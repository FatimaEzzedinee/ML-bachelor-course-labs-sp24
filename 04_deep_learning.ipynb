{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y3Zazur7WcWP"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatimaEzzedinee/ML-bachelor-course-labs-sp24/blob/main/04_deep_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IN5IXHc6WcWQ"
      },
      "source": [
        "# Machine Learning SP 2023/2024\n",
        "\n",
        "- Prof. Cesare Alippi\n",
        "- Alvise Dei Rossi ([`alvise.dei.rossii@usi.ch`](mailto:alvise.dei.rossi@usi.ch))<br>\n",
        "- Fatima Ezzeddine ([`fatima.ezzeddine@usi.ch`](mailto:fatima.ezzeddine@usi.ch))<br>\n",
        "- Alessandro Manenti ([`alessandro.manenti@usi.ch`](mailto:alessandro.manenti@usi.ch))\n",
        "\n",
        "---\n",
        "# Lab 04: Deep Learning\n",
        "\n",
        "___\n",
        "\n",
        "In this lab we will see:\n",
        "\n",
        "0. Theoretical recap of neural networks;\n",
        "1. How to implement, train and evaluate a Fully Connected Neural Network (FCNN) for **regression** with `PyTorch`;\n",
        "2. How to implement, train and evaluate a Convolutional Neural Network (CNN) for **classification** with `PyTorch`.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-lXvxl-WcWQ"
      },
      "source": [
        "# 0. Theoretical Recap of Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xenq0SbxWcWR"
      },
      "source": [
        "The core component of neural networks is the neuron, which is composed of a perceptron and an activation function:\n",
        "\n",
        "$$\n",
        "f(x; \\boldsymbol \\theta) =  h( x^T \\boldsymbol \\theta).\n",
        "$$\n",
        "\n",
        "The main idea behind neural networks is to compose neurons in two different ways:\n",
        "\n",
        "1. by taking many neurons __in parallel__;\n",
        "2. by composing many subsequent __layers__ of neurons;\n",
        "\n",
        "The result is a network of neurons that take data as input, and compute sequential transformations until the desired result is produced as output.\n",
        "\n",
        "![alt text](https://res.cloudinary.com/practicaldev/image/fetch/s--4XiAvCCB--/c_limit%2Cf_auto%2Cfl_progressive%2Cq_auto%2Cw_880/https://cdn-images-1.medium.com/max/1200/1%2AYgJ6SYO7byjfCmt5uV0PmA.png)\n",
        "\n",
        "---\n",
        "\n",
        "We can write the output of the hidden layer as:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "h_0 \\\\\n",
        "h_1 \\\\\n",
        "h_2 \\\\\n",
        "\\vdots\\\\\n",
        "h_l\n",
        "\\end{bmatrix}\n",
        "=\n",
        "h\\left(\n",
        "\\begin{bmatrix}\n",
        "w_{00} & w_{01} & w_{02} & \\cdots & w_{0m} \\\\\n",
        "w_{10} & w_{11} & w_{12} & \\cdots & w_{1m} \\\\\n",
        "w_{20} & w_{21} & w_{22} & \\cdots & w_{2m} \\\\\n",
        "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "w_{l0} & w_{l1} & w_{l2} & \\cdots & w_{lm} \\\\\n",
        "\\end{bmatrix}\n",
        "\\begin{bmatrix}\n",
        "x_0 \\\\\n",
        "x_1 \\\\\n",
        "x_2 \\\\\n",
        "\\vdots\\\\\n",
        "x_m\n",
        "\\end{bmatrix}\n",
        "+\n",
        "\\begin{bmatrix}\n",
        "b_0 \\\\\n",
        "b_1 \\\\\n",
        "b_2 \\\\\n",
        "\\vdots\\\\\n",
        "b_l\n",
        "\\end{bmatrix}\n",
        "\\right)\n",
        "$$\n",
        "\n",
        "In short, we write the output of a __layer__ of neurons as:\n",
        "$$\n",
        "H = h(Wx + b_w)\n",
        "$$\n",
        "\n",
        "_NB: without the activation function a layer is a simple affine trasformation._\n",
        "\n",
        "We can compute the output of the network doing the same calculation for the  \"Output\" neurons, with the difference that their input is not $X$, as for the hidden neurons, but it is the output $H$ of the last hidden layer. The output layer can be written as:\n",
        "\n",
        "$$\n",
        "Y = \\sigma(VH + b_v)\n",
        "$$\n",
        "\n",
        "(note that $V$ is a different matrix of parameters).\n",
        "\n",
        "Finally, stacking the two layers simply means __composing__ them together, so that the whole neural network can be written as:\n",
        "\n",
        "$$\n",
        "\\hat y = f(x;\\boldsymbol \\theta = \\{W, b_w, V, b_v\\}) = \\sigma\\left(V h(Wx + b_w)  + b_v\\right)\n",
        "$$\n",
        "\n",
        "---\n",
        "Neural networks are trained with __stochastic gradient descent__ (SGD). The key idea behind SGD is to update all the parameters of the network at the same time, based on how each parameter contributed to the __loss__ function $L( \\boldsymbol \\theta)$.\n",
        "\n",
        "The generalized update rule reads:\n",
        "\n",
        "$$\n",
        "{\\boldsymbol \\theta}^{i+1} = {\\boldsymbol \\theta}^{i} - \\varepsilon \\frac{\\partial L({\\boldsymbol \\theta})}{\\partial {\\boldsymbol \\theta}}\\bigg\\vert_{{\\boldsymbol \\theta} = {\\boldsymbol \\theta}^i}\n",
        "$$\n",
        "\n",
        "where $\\varepsilon$ is again called __learning rate__.\n",
        "\n",
        "At each iteration:\n",
        "\n",
        "1. We take a **batch** ($x_n$, $y_n$) of samples from the dataset;\n",
        "2. We compute the **output** $\\hat ($y_n$) of the network for each sample in the batch;\n",
        "3. We compute an empirical estimate of the **loss** $L$ on the batch;\n",
        "4. We compute the **gradient** of the loss with respect to the parameters of the network (**backpropagation**);\n",
        "5. We update the parameters of the network with the **optimizer** rule (e.g., SGD, Adam, RMSprop, etc.).\n",
        "\n",
        "Look at Tutorial 1 to refresh your memory on Gradient Descent and Backpropagation. There we implemented the gradients of the loss function with respect to the parameters of the network by hand. In this lab we will use `PyTorch` to do this for us.\n",
        "\n",
        "---\n",
        "When training neural networks for **regression**, we often take the loss to be the __mean squared error function__:\n",
        "\n",
        "$$\n",
        "L({\\boldsymbol \\theta}) =  \\frac{1}{N} \\sum_{n=1}^N \\left(y_n - \\hat y_n\\right)^2\n",
        "$$\n",
        "\n",
        "or the __mean absolute error function__:\n",
        "\n",
        "$$\n",
        "L({\\boldsymbol \\theta}) =  \\frac{1}{N} \\sum_{n=1}^N \\left|y_n - \\hat y_n\\right|\n",
        "$$\n",
        "\n",
        "\n",
        "When training neural networks for **classification**, we often take the loss to be the __cross-entropy error function__:\n",
        "\n",
        "$$\n",
        "L({\\boldsymbol \\theta}) =  -\\frac{1}{N} \\sum_{n=1}^N \\sum_{c=1}^C y_{nc} \\log(\\hat y_{nc})\n",
        "$$\n",
        "\n",
        "where $C$ is the number of classes, and $y_{nc}$ is the true label of the $n$-th sample for the $c$-th class.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUg_P750WcWR"
      },
      "source": [
        "# A) Fully Connected Neural Network (FCNN) for Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxmpVYsjWcWR"
      },
      "source": [
        "To build our neural network we will use [PyTorch](https://pytorch.org/), one of the most popular deep learning libraries for Python (others being [JAX](https://jax.readthedocs.io/en/latest/notebooks/quickstart.html) and [TensorFlow](https://www.tensorflow.org/)).\n",
        "\n",
        "PyTorch provides a huge number of functions, like Numpy, that can be used to manipulate arrays, but offers a great advantages w.r.t. Numpy:  the library implements __automatic differentiation__, meaning that the most analytically complex step of training, the computation of the gradient, is handled for you.\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c6/PyTorch_logo_black.svg/2560px-PyTorch_logo_black.svg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "56OSJPZgWcWR"
      },
      "source": [
        "## A.1) Collect and prepare the data\n",
        "\n",
        "Let's start with a toy regression problem.\n",
        "\n",
        "To use PyTorch, you'll need to import the necessary libraries. The standard are:\n",
        "- `torch.nn` contains various neural network modules\n",
        "- `torch.optim` provides optimization algorithms."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy6IIM3wYfYI",
        "outputId": "bda18d05-59a6-4327-ad70-e784bf4ca401"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# # We can seeif a GPU is available\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# print(device)\n",
        "\n",
        "# # For Mac users you can try\n",
        "device = torch.device('mps' if torch.backends.mps.is_available() else \"cpu\") \n",
        "print(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "d2GZiA2SWcWR",
        "outputId": "4d0853de-4bb0-4481-b086-3d462de98822"
      },
      "outputs": [],
      "source": [
        "# Create dataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "Ct = '#fa9352'\n",
        "Cv = '#527afa'\n",
        "\n",
        "def ground_truth(x):\n",
        "    return np.tanh(5*x**5 + np.sinc(2*x) - 0.2*np.sin((x-0.5)*5)/(x-0.5) - np.exp(-(x**2)/0.02)) # Very complex function\n",
        "\n",
        "def generate_data(n_samples=512, seed=42):\n",
        "    np.random.seed(seed)\n",
        "    x = np.random.rand(n_samples) * 2 - 1\n",
        "    # x = np.linspace(-1, 1, n_samples)\n",
        "    y = ground_truth(x) + np.random.randn(n_samples) * 0.1\n",
        "    return torch.tensor(x, dtype=torch.float).reshape(-1, 1), torch.tensor(y, dtype=torch.float).reshape(-1, 1)\n",
        "\n",
        "ground_truth_x = torch.linspace(-1, 1, 1000).reshape(-1, 1)\n",
        "ground_truth_y = ground_truth(ground_truth_x)\n",
        "plt.plot(ground_truth_x, ground_truth_y, label='Ground Truth', color='g')\n",
        "\n",
        "x_train, y_train = generate_data(n_samples=64, seed=0)\n",
        "x_val, y_val = generate_data(n_samples=32, seed=1)\n",
        "x_test, y_test = generate_data(n_samples=32, seed=2)\n",
        "\n",
        "plt.scatter(x_train, y_train, s=5, label='Train', color=Ct)\n",
        "plt.scatter(x_val, y_val, s=5, label='Validation', color=Cv)\n",
        "plt.scatter(x_test, y_test, s=5, label='Test', color='k')\n",
        "plt.grid(alpha=0.5)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "465MzlkhWcWT"
      },
      "source": [
        "## A.2) Define the network\n",
        "\n",
        "To build a neural network in PyTorch, you need to define a class that inherits from `torch.nn.Module`. This class should have two main methods:\n",
        "- `__init__` method, where you define the layers of the network;\n",
        "- `forward` method, where you define the forward pass of the network."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZEDCRuTgWcWT"
      },
      "outputs": [],
      "source": [
        "class FCNN(nn.Module):\n",
        "    def __init__(self):     # Here we define the structure of the network\n",
        "        super(FCNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(1, 2)\n",
        "        self.fc2 = nn.Linear(2, 1)\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):   # Here we define the forward pass\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "model = FCNN().to(device) # Create the model and move to device (GPU or CPU)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bYxrrxAhWcWT"
      },
      "source": [
        "Voil√†! You have a neural network.\n",
        "\n",
        "How do you think it performs on the data?\n",
        "\n",
        "Well? Why / Why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "w3Ty_89wWcWT",
        "outputId": "7c9cea7b-d287-4414-b200-683277facaea"
      },
      "outputs": [],
      "source": [
        "y_pred = model(ground_truth_x.to(device)) # Forward pass\n",
        "\n",
        "plt.scatter(x_train, y_train, label='Train', color=Ct)\n",
        "plt.plot(ground_truth_x, y_pred.detach().cpu(), label='Model Predictions', color='r')\n",
        "plt.plot(ground_truth_x, ground_truth_y, label='Ground Truth', color='g')\n",
        "plt.grid(alpha=0.5)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kn5GsIeOWcWT"
      },
      "source": [
        "## A.3) Train the network\n",
        "\n",
        "To train the network, you need to:\n",
        "- Define an optimizer (e.g., `torch.optim.SGD`, `torch.optim.Adam`, etc.);\n",
        "- Define a loss function (e.g., `torch.nn.MSELoss`, `torch.nn.L1Loss`, etc.);\n",
        "- Loop over the dataset, and for each batch:\n",
        "    - Compute the output of the network;\n",
        "    - Compute the loss;\n",
        "    - Compute the gradients of the loss with respect to the parameters of the network;\n",
        "    - Update the parameters of the network with the optimizer.\n",
        "\n",
        "    \n",
        "We will use the `torch.optim.Adam` optimizer and the `torch.nn.MSELoss` loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "Cmy7nFnLWcWT",
        "outputId": "9b295c70-7e40-4a3e-bc7b-2e13270567bf"
      },
      "outputs": [],
      "source": [
        "# Reinitalize the model\n",
        "model = FCNN().to(device)\n",
        "\n",
        "# Training\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "training_losses = []\n",
        "\n",
        "n_epochs = 100\n",
        "# Training loop\n",
        "for epoch in range(n_epochs):\n",
        "    optimizer.zero_grad()               # Zero the gradients\n",
        "\n",
        "    y_pred = model(x_train.to(device))             # Forward pass\n",
        "\n",
        "    loss = criterion(y_pred, y_train.to(device))   # Compute the loss\n",
        "    loss.backward()                     # Compute the gradients\n",
        "    optimizer.step()                    # Update the weights\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch: {epoch} - Loss: {loss.item()}')\n",
        "    training_losses.append(loss.item()) # Save the loss for plotting\n",
        "\n",
        "plt.plot(training_losses, label='Training Loss')\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "3iivbIMdWcWT",
        "outputId": "f1d56dc0-39ac-4636-aaaa-5bbbd1db6230"
      },
      "outputs": [],
      "source": [
        "# Plot the model predictions\n",
        "y_pred = model(ground_truth_x.to(device)) # Forward pass\n",
        "plt.scatter(x_train, y_train, label='Train', color=Ct)\n",
        "plt.plot(ground_truth_x, y_pred.detach().cpu(), label='Model Predictions', color='r')\n",
        "plt.plot(ground_truth_x, ground_truth_y, label='Ground Truth', color='g')\n",
        "plt.grid(alpha=0.5)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSLw5FYDWcWT"
      },
      "source": [
        "Is it better? Now try to:\n",
        "1. Change `n_epochs`\n",
        "2. Change `lr`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifmpT629WcWT"
      },
      "source": [
        "### Exercise: Bigger model\n",
        "\n",
        "Now, let's try to build a bigger model, in order to do so modify the `__init__` method of the `FCNN` class to allow for a different number of neurons in the hidden layer.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "73Cy3s7XWcWT"
      },
      "outputs": [],
      "source": [
        "class FCNN(nn.Module):\n",
        "    def __init__(...):     # Here we define the structure of the network\n",
        "        super(FCNN, self).__init__()\n",
        "        self.fc1 = ...\n",
        "        self.fc2 = ...\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):   # Here we define the forward pass\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB4_k6GjWcWT"
      },
      "source": [
        "### Solution:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xe-eW586WcWT"
      },
      "outputs": [],
      "source": [
        "class FCNN(nn.Module):\n",
        "    def __init__(self, hidden_dim):     # Here we define the structure of the network\n",
        "        super(FCNN, self).__init__()\n",
        "        self.fc1 = nn.Linear(1, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, 1)\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):   # Here we define the forward pass\n",
        "        x = self.activation(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Cq_07o7WcWT"
      },
      "source": [
        "### Train loop\n",
        "\n",
        "As we are building a bigger model, **can we use the same training loop or we should monitor something else?** Why/Why not?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yzFjI8sbWcWT"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, x_train, y_train, x_val, y_val, n_epochs, eval_freq, device):\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()                       # Set the model to training mode, this is important for some layers (e.g. dropout)\n",
        "        optimizer.zero_grad()               # Zero the gradients\n",
        "\n",
        "        y_pred = model(x_train.to(device))             # Forward pass\n",
        "\n",
        "        loss = criterion(y_pred, y_train.to(device))   # Compute the loss\n",
        "        loss.backward()                     # Compute the gradients\n",
        "        optimizer.step()                    # Update the weights\n",
        "\n",
        "        training_losses.append(loss.item()) # Save the loss for plotting\n",
        "\n",
        "        # Validation\n",
        "        model.eval()                        # Set the model to evaluation mode\n",
        "        if epoch % eval_freq == 0:\n",
        "            with torch.no_grad():            # No need to compute the gradients\n",
        "                y_val_pred = model(x_val.to(device))\n",
        "                val_loss = criterion(y_val_pred, y_val.to(device))\n",
        "                validation_losses.append(val_loss.item())\n",
        "\n",
        "            print(f'Epoch {epoch}, Train Loss {loss.item()}')\n",
        "            print(f'Validation Loss {val_loss.item()}')\n",
        "            print('')\n",
        "    return training_losses, validation_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eq5hWgnCWcWU",
        "outputId": "1e185b7a-28eb-44fb-b59f-605f81ea5138"
      },
      "outputs": [],
      "source": [
        "# Reinitalize the model\n",
        "model = FCNN(hidden_dim=32).to(device) # Create the model and move to device (GPU or CPU)\n",
        "\n",
        "# Training\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "eval_freq = 100\n",
        "n_epochs = 1000\n",
        "# Let's train the model\n",
        "training_losses, validation_losses = train(model, criterion, optimizer, x_train, y_train, x_val, y_val, n_epochs, eval_freq, device)\n",
        "\n",
        "plt.plot(training_losses, label='Training Loss', color=Ct)\n",
        "plt.plot(np.arange(0, n_epochs, 100), validation_losses, label='Validation Loss', color=Cv)\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "# Plot the model predictions\n",
        "y_pred = model(ground_truth_x.to(device)) # Forward pass\n",
        "plt.scatter(x_train, y_train, label='Train', color=Ct)\n",
        "plt.plot(ground_truth_x, y_pred.detach().cpu(), label='Model Predictions', color='r')\n",
        "plt.plot(ground_truth_x, ground_truth_y, label='Ground Truth', color='g')\n",
        "plt.grid(alpha=0.5)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SHR8QAuxWcWU"
      },
      "source": [
        "With almost zero effort or knowledge we achieved pretty good results! N.B. this is not always the case, Deep Learning is not magic, but it is a powerful tool."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALqOK_hLWcWU"
      },
      "source": [
        "### Exercise 1: Change the activation function to ReLU and see what happens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7avsn1qWcWU"
      },
      "source": [
        "### Exercise 2: Arbitrary big model\n",
        "\n",
        "Go back and look at the definition of the model. How would you change it to have an arbitrary number of hidden layers and neurons in each layer?\n",
        "\n",
        "Usefull if you want to try different configurations of the model, without having to write a new class for each configuration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ifwuJXJpWcWU"
      },
      "source": [
        "## A.4) Arbitrarly big model with `nn.ModuleList`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kHqXbVPNWcWU"
      },
      "source": [
        "`nn.ModuleList` is a container for modules that can be accessed by index (similar to Python lists).\n",
        "\n",
        " It's useful when you need to create and manage a variable number of modules, or when the forward pass through the network involves more complex, non-sequential operations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIhu8PwWWcWU"
      },
      "outputs": [],
      "source": [
        "class FCNN(nn.Module):\n",
        "    def __init__(self, dims):\n",
        "        super(FCNN, self).__init__()\n",
        "        self.layers = nn.ModuleList()\n",
        "        for i in range(len(dims)-1):\n",
        "            self.layers.append(nn.Linear(dims[i], dims[i+1]))\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        for layer in self.layers[:-1]:\n",
        "            x = self.activation(layer(x))\n",
        "        x = self.layers[-1](x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "k6HJhMlqWcWV",
        "outputId": "2ceac9f9-ba12-4ea9-c609-7a19a47b0887"
      },
      "outputs": [],
      "source": [
        "# Let's train the model\n",
        "dims = [1, 4, 4, 4, 1]\n",
        "n_epochs = 10000\n",
        "\n",
        "model = FCNN(dims).to(device)\n",
        "\n",
        "# Training\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "\n",
        "training_losses, validation_losses = train(model, criterion, optimizer, x_train, y_train, x_val, y_val, n_epochs, eval_freq, device)\n",
        "\n",
        "plt.plot(training_losses, label='Training Loss', color=Ct)\n",
        "plt.plot(np.arange(0, n_epochs, 100), validation_losses, label='Validation Loss', color=Cv)\n",
        "\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.5)\n",
        "plt.show()\n",
        "\n",
        "# Plot the model predictions\n",
        "y_pred = model(ground_truth_x.to(device)) # Forward pass\n",
        "plt.scatter(x_train, y_train, label='Train', color=Ct)\n",
        "plt.plot(ground_truth_x, y_pred.detach().cpu(), label='Model Predictions', color='r')\n",
        "plt.plot(ground_truth_x, ground_truth_y, label='Ground Truth', color='g')\n",
        "plt.grid(alpha=0.5)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HV6cGjSmWcWV"
      },
      "source": [
        "Now experiment with it to gain confidence with PyTorch and neural networks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZAvTT2uWcWV"
      },
      "source": [
        "# B) Convolutional Neural Network (CNN) for Classification\n",
        "\n",
        "In this part we will explore how to build a Convolutional Neural Network (CNN) for a classification task.\n",
        "\n",
        "We will also explore three important concepts in Deep Learning:\n",
        "- `torch.utils.data.Dataset` **and** `torch.utils.data.DataLoader`: classes to load and preprocess the data in torch;\n",
        "- **Unbalanced Datasets**: a common problem in classification tasks, where the number of samples in each class is not equal.\n",
        "- **Data Augmentation**: a technique to artificially increase the size of the training set by applying transformations to the input data;\n",
        "- **Early Stopping**: a technique to prevent overfitting by stopping the training when the validation loss increases for a certain number of epochs.\n",
        "\n",
        "\n",
        "We will use the `torchvision` library to load and preprocess the data.\n",
        "\n",
        "But first, a brief recap on CNNs.\n",
        "\n",
        "![alt text](https://upload.wikimedia.org/wikipedia/commons/6/63/Typical_cnn.png)\n",
        "\n",
        "\n",
        "CNNs were first introduced by Kunihiko Fukushima in 1980, and were later popularized by Y. LeCun, when he successfully applied backpropagation to train CNNs on MNIST.\n",
        "\n",
        "In CNNs, we use our **prior knowledge** about the problem (i.e., the data are translational invariant), imposing that neurons **share** some weights (i.e., the convolutional kernels).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPy07uBOWcWV"
      },
      "source": [
        "# B.1) Load and preprocess the data\n",
        "\n",
        "We will use the CIFAR-10 dataset, which consists of 60,000 32x32 color images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIYLGfYDWcWV"
      },
      "source": [
        "### B.1.1) Downoad the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "id": "TQF-a13vWcWV",
        "outputId": "bd08482c-3d20-4847-eb60-356b877344c8"
      },
      "outputs": [],
      "source": [
        "# Download the dataset\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Define the transformations to apply to the images and download the dataset\n",
        "transformations = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]) # The output of torchvision datasets are PILImage images of range [0, 1]. We transform them to Tensors of normalized range [-1, 1].\n",
        "cifar10_train = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transformations)\n",
        "cifar10_test = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transformations)\n",
        "# Split in test and validation\n",
        "cifar10_val, cifar10_test = torch.utils.data.random_split(cifar10_test, [5000, 5000])\n",
        "\n",
        "# Plot 9 random images from the dataset\n",
        "def plot_9_random_images(dataset, with_predictions=False, model=None):\n",
        "    class_label_map = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "    fig, axes = plt.subplots(3, 3, figsize=(10, 10))                            # Create a figure and a grid of subplots\n",
        "    for i in range(3):\n",
        "        for j in range(3):\n",
        "            random_indeces = np.random.randint(len(dataset))            # Get 9 random indeces\n",
        "            x, y = dataset[random_indeces]\n",
        "            axes[i, j].imshow(x.permute(1, 2, 0).numpy() * 0.5 + 0.5)   # Rearrange the dimensions of the tensor from (C, H, W) to (H, W, C),\n",
        "                                                                        # which is the expected format for Matplotlib's imshow function.\n",
        "                                                                        # The * 0.5 + 0.5 operation is used to unnormalize the pixel values\n",
        "            if with_predictions:\n",
        "                axes[i, j].set_title(f'Class: {class_label_map[y]}, Pred: {class_label_map[model(x).argmax().item()]}')\n",
        "            else:\n",
        "                axes[i, j].set_title(f'Class: {class_label_map[y]}')\n",
        "            axes[i, j].axis('off')\n",
        "    plt.show()\n",
        "\n",
        "plot_9_random_images(cifar10_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aFRY0CjkWcWV"
      },
      "source": [
        "Now we make the dataset unbalanced by removing some samples from the classes \"frog\" and \"truck\".\n",
        "\n",
        "An unbalanced dataset is a dataset where the number of samples in each class is not equal (usually very different). This is a common problem in classification tasks, and can lead to poor performance of the classifier.\n",
        "\n",
        "Why do you think this is bad?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlPz5YToWcWV"
      },
      "outputs": [],
      "source": [
        "# Remove some samples from the \"cat\" and \"truck\" classes in the training set with probability P\n",
        "P = 0.2\n",
        "unbalanced_train_data = [(x, y) for x, y in cifar10_train if y not in [3, 9]]\n",
        "unbalanced_train_data += [(x, y) for x, y in cifar10_train if y in [3, 9] and torch.rand(1) < P]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aksQgHY2WcWW"
      },
      "source": [
        "### B.1.2) Data Augmentation\n",
        "\n",
        "Data augmentation is a technique to artificially increase the size of the training set by applying transformations to the input data. This can help the model generalize better to unseen data.\n",
        "\n",
        "We will use the `torchvision.transforms` module to apply some transformations to the input data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B3VnGVAJWcWW"
      },
      "outputs": [],
      "source": [
        "data_augmentation_transform = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),  # Horizontal flip with probability 0.5\n",
        "    transforms.RandomRotation(10),      # Rotate the image by a random angle between -10 and 10 degrees\n",
        "    transforms.RandomResizedCrop((32, 32), scale=(0.8, 1.0))  # Take a random crop of the original image with an area between 80% to 100% of the original image\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puZnEMmpWcWW"
      },
      "source": [
        "### B.1.3) `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQuvhKWlWcWW"
      },
      "source": [
        "Now that we have the data as a list and the data augmentation transformation, we can use the `torch.utils.data.Dataset` class to create a PyTorch dataset. Look [here](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html) for a deeper tutorial on how to use `torch.utils.data.Dataset` and `torch.utils.data.DataLoader`.\n",
        "\n",
        "A `Dataset` is a Python class that represents a dataset. It should implement three methods:\n",
        "\n",
        "- `__init__`: where you specify how or where to load the dataset;\n",
        "- `__getitem__`: where you specify how to get the $i$-th sample of the dataset;\n",
        "- `__len__`: where you specify how to get the length of the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "2K7rJfMLWcWW",
        "outputId": "9c629c05-1eae-4e1e-cabb-86a6500b659f"
      },
      "outputs": [],
      "source": [
        "# Create new Dataset objects for the unbalanced training and test datasets\n",
        "class UnbalancedDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data, transform=None):\n",
        "        self.data = data\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x, y = self.data[index]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "unbalanced_dataset = UnbalancedDataset(unbalanced_train_data, data_augmentation_transform) # The data augmentation is only applied whenever a sample is requested\n",
        "\n",
        "# Plot 9 random images from the unbalanced dataset\n",
        "plot_9_random_images(unbalanced_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHsXhMa-WcWW"
      },
      "source": [
        "`Dataset` are used to define how to load and preprocess the data, while `DataLoader` are used to actually load the data in batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7JD8pWFbWcWX",
        "outputId": "9a893d5d-1e14-4f66-a030-545aaddaa226"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 64\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(unbalanced_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "# The dataloader can be used as an iterator\n",
        "for x, y in train_loader:\n",
        "    print(x.shape, y.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmApki8NWcWX"
      },
      "source": [
        "## B.2) Define the CNN\n",
        "\n",
        "As before, to build a CNN in PyTorch, we define a class that inherits from `torch.nn.Module`. This class has the same two main methods:\n",
        "\n",
        "- `__init__` method, where you define the layers of the network;\n",
        "- `forward` method, where you define the forward pass of the network.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uWPf_60fWcWX"
      },
      "outputs": [],
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, padding=1)  # Can we change the number of output channels?\n",
        "        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(in_features=64 * 4 * 4, out_features=128)\n",
        "        self.fc2 = nn.Linear(in_features=128, out_features=10)\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.activation(self.conv1(x)))\n",
        "        x = self.pool(self.activation(self.conv2(x)))\n",
        "        x = self.pool(self.activation(self.conv3(x)))\n",
        "\n",
        "        x = x.view(-1, 64 * 4 * 4)  # Flatten the tensor\n",
        "\n",
        "        x = self.activation(self.fc1(x))\n",
        "\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x # We don't apply the softmax here because it is included in the torch implementation of the cross entropy loss function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfhZ9XxQWcWX"
      },
      "source": [
        "## B.3) Unbalanced Datasets\n",
        "\n",
        "How would you try to solve the problem of unbalanced datasets?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 481
        },
        "id": "o3_Yn5QjWcWX",
        "outputId": "42717630-e628-4d3f-8e8c-adce022692ef"
      },
      "outputs": [],
      "source": [
        "# Let's see how unbalanced the dataset is\n",
        "class_label_map = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "class_counter = torch.zeros(10)\n",
        "for _, y in unbalanced_train_data:\n",
        "    class_counter[y] += 1\n",
        "\n",
        "plt.bar(np.arange(10), class_counter)\n",
        "plt.xticks(np.arange(10), class_label_map, rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47IWXiCyWcWX"
      },
      "source": [
        "\n",
        "There are various techniques to deal with unbalanced datasets, such as:\n",
        "\n",
        "- **Resampling**: This involves either oversampling the minority class or undersampling the majority class to balance the dataset.\n",
        "- **Data Augmentation**: This involves artificially increasing the size of the training set by applying transformations to the input data.\n",
        "- **Weighted Loss**: This involves assigning different weights to the classes in the loss function, so that the model pays more attention to the minority class.\n",
        "\n",
        "We will use the **Weighted Loss** technique.\n",
        "\n",
        "\n",
        "The trick consists in __re-weighting__ the loss function, so that the error on rare samples will count more than the error on common samples:\n",
        "\n",
        "$$\n",
        "L_{\\textrm{reweighted}}(y, f(X; W)) =\n",
        "\\begin{cases}\n",
        "\\lambda_0 L(y, f(X; W))\\textrm{, if } y=0 \\\\\n",
        "\\lambda_1 L(y, f(X; W))\\textrm{, if } y=1\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Ideally, $\\lambda_0$ and $\\lambda_1$ should represent how rare the respective classes are in the dataset.\n",
        "A common way of computing the two values automatically is as:\n",
        "\n",
        "$$\n",
        "\\lambda_i = \\frac{\\textrm{\\# samples in dataset}}{\\textrm{\\# classes}\\cdot\\textrm{\\# samples of class } i}\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJMlHRs5WcWY"
      },
      "outputs": [],
      "source": [
        "lambda_i = class_counter.sum() / (10 * class_counter)\n",
        "\n",
        "criterion = torch.nn.CrossEntropyLoss(weight=lambda_i).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rbelaf36WcWY"
      },
      "source": [
        "## B.4) Train the CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KQzv1FWWcWY"
      },
      "outputs": [],
      "source": [
        "def train(model, criterion, optimizer, train_loader, val_loader, n_epochs, eval_freq, device):\n",
        "    training_losses = []\n",
        "    validation_losses = []\n",
        "\n",
        "    for epoch in range(n_epochs):\n",
        "        model.train()                       # Set the model to training mode, this is important for some layers (e.g. dropout)\n",
        "\n",
        "        for x, y in train_loader:           # Access the training data\n",
        "            optimizer.zero_grad()           # Zero the gradients\n",
        "            x, y = x.to(device), y.to(device)\n",
        "            y_pred = model(x)               # Forward pass\n",
        "\n",
        "            loss = criterion(y_pred, y)\n",
        "            loss.backward()                 # Compute the gradients\n",
        "            optimizer.step()                # Update the weights\n",
        "\n",
        "            training_losses.append(loss.item()) # Save the loss for plotting\n",
        "\n",
        "        # Validation\n",
        "        model.eval()                        # Set the model to evaluation mode\n",
        "        if epoch % eval_freq == 0:\n",
        "            with torch.no_grad():            # No need to compute the gradients\n",
        "                val_loss = 0\n",
        "                for x, y in val_loader:\n",
        "                    x, y = x.to(device), y.to(device)\n",
        "                    y_val_pred = model(x)\n",
        "                    val_loss += criterion(y_val_pred, y)\n",
        "                validation_losses.append(val_loss.item() / len(val_loader))\n",
        "\n",
        "            print(f'Epoch {epoch}, Train Loss {training_losses[-1]:.4f}')\n",
        "            print(f'Validation Loss {validation_losses[-1]:.4f}')\n",
        "            print('')\n",
        "\n",
        "    return training_losses, validation_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaS_93tUWcWY",
        "outputId": "c5e874aa-699c-4a0c-d249-c7a90eee18fe"
      },
      "outputs": [],
      "source": [
        "# Reinitalize the model\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Training\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "eval_freq = 1\n",
        "n_epochs = 3\n",
        "\n",
        "training_losses, validation_losses = train(model, criterion, optimizer, train_loader, val_loader, n_epochs, eval_freq, device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "48iyuRMBWcWY",
        "outputId": "2ec6b1ea-85af-400b-eebd-945f88ef6abd"
      },
      "outputs": [],
      "source": [
        "# Test the accuracy on the test set\n",
        "test_loader = torch.utils.data.DataLoader(cifar10_test, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for x, y in test_loader:\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        y_pred = model(x)\n",
        "        _, predicted = torch.max(y_pred, dim=1)\n",
        "        total += y.size(0)\n",
        "        correct += (predicted == y).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct / total}%')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 829
        },
        "id": "aZroq4NhWcWY",
        "outputId": "446827db-4784-4090-9f00-2e981503814a"
      },
      "outputs": [],
      "source": [
        "plot_9_random_images(cifar10_test, with_predictions=True, model=model.cpu())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZRgDg5dWcWY"
      },
      "source": [
        "As an exercise, try to:\n",
        "\n",
        "1. Plot the confusion matrix of the model on the test set\n",
        "2. Use the whole dataset and not the unbalanced version we created\n",
        "3. Change the learning rate and the number of training epochs\n",
        "4. Change the model (maybe look the Dropout regularization technique)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
