{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatimaEzzedinee/ML-bachelor-course-labs-sp24/blob/main/03_model_performance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwPfDFnz1ECg"
      },
      "source": [
        "# Machine Learning SP 2023/2024\n",
        "\n",
        "- Prof. Cesare Alippi\n",
        "- Alvise Dei Rossi ([`alvise.dei.rossi@usi.ch`](mailto:alvise.dei.rossi@usi.ch))<br>\n",
        "- Fatima Ezzeddine ([`fatima.ezzeddine@usi.ch`](mailto:fatima.ezzeddine@usi.ch))<br>\n",
        "- Alessandro Manenti ([`alessandro.manenti@usi.ch`](mailto:alessandro.manenti@usi.ch))\n",
        "\n",
        "---\n",
        "\n",
        "# Lab 03: Model Performance\n",
        "\n",
        "The objectives of the lab are as follows:\n",
        "\n",
        "- Evaluate the performance of a model\n",
        "- Perform splits on the data\n",
        "- Assess which model is the best along many models\n",
        "- Gain familiarity with the concept of models hyper-parameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-s8G3s6HzttP"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YMfbdgMUQvJX"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- Auxiliary code -------------------- #\n",
        "\n",
        "# function to plot decision boundaries\n",
        "def plot_decision_surface(model, x, y, transform=lambda x:x, title=\"\"):\n",
        "\n",
        "  from matplotlib.colors import ListedColormap\n",
        "  # color_maps\n",
        "  cm = plt.cm.RdBu\n",
        "  cols = ['#FF0000', '#0000FF']\n",
        "  cm_bright = ListedColormap(cols)\n",
        "\n",
        "  #init figure\n",
        "  fig = plt.figure()\n",
        "\n",
        "  # Create mesh\n",
        "  h = .1  # step size in the mesh\n",
        "  x_min, x_max = x[:, 0].min() - .5, x[:, 0].max() + .5\n",
        "  y_min, y_max = x[:, 1].min() - .5, x[:, 1].max() + .5\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                       np.arange(y_min, y_max, h))\n",
        "\n",
        "  # plot train data\n",
        "  cy = [cols[int(yi)] for yi in y] # list of color for every observation based on its class\n",
        "  plt.scatter(x[:, 0], x[:, 1], c=cy, cmap=cm_bright,\n",
        "              edgecolors='k')\n",
        "  plt.xlim(xx.min(), xx.max())\n",
        "  plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "  plt.xlabel(r'$x_1$')\n",
        "  plt.ylabel(r'$x_2$')\n",
        "  plt.title(title)\n",
        "\n",
        "  y_pred = model.predict(transform(np.c_[xx.ravel(), yy.ravel()])) # predict for every point in the mesh\n",
        "  # note that we predict the class directly, we don't use predict_proba\n",
        "\n",
        "  y_pred = y_pred.reshape(xx.shape)\n",
        "  plt.contourf(xx, yy, y_pred > 0.5, cmap=cm, alpha=.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klMKmWT8KpcD"
      },
      "source": [
        "## Use-case scenario\n",
        "Consider the following use-case scenario: A company has approached us with a request to develop a machine learning model for one of their machines. We have been provided with a labeled dataset consisting of $(x_i, y_i)$ for $i=1, ..., N$.\n",
        "\n",
        "Our objective is to identify the best possible model, denoted as $f(x; \\hat \\theta)$, and provide an estimate of its performance, which is represented as $V(\\hat \\theta)$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1fYESi8MXe4"
      },
      "outputs": [],
      "source": [
        "# Prepare some data\n",
        "N = 400\n",
        "\n",
        "np.random.seed(42)\n",
        "# generating data points by sampling from a random distribution and then changing the means of the distributions by adding constants to the values\n",
        "Xa = np.random.randn(N//4, 2) # second cluster\n",
        "Xb = np.random.randn(N//4, 2) + np.array([ 8.,  1.]) # forth cluster\n",
        "Xc = np.random.randn(N//4, 2) + np.array([-4., -1.]) # first cluster\n",
        "Xd = np.random.randn(N//4, 2) + np.array([ 4., -1.]) # third cluster\n",
        "\n",
        "#stacking the features together\n",
        "X = np.vstack([Xa, Xb, Xc, Xd])\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezNtzG5hzttQ"
      },
      "source": [
        "Creates two classes of equal size, one consisting of the first half of the elements in y (which are all zero) and the other consisting of the second half of the elements in y (which are all one)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtEtGAdPMxRE"
      },
      "outputs": [],
      "source": [
        "# Assigning labels to our dataset\n",
        "\n",
        "# Creates a NumPy array y of size N with all elements initialized to zero.\n",
        "y = np.zeros((N,)) # First half of the elements in y will be class zero. (clusters 2 and 4)\n",
        "\n",
        "# Sets the second half of the elements in y to one. (clusters 1 and 3)\n",
        "y[N//2:] = 1\n",
        "\n",
        "# plot the data\n",
        "plt.scatter(X[:N//2, 0], X[:N//2, 1], c=\"red\", label=\"class 0\")\n",
        "plt.scatter(X[N//2:, 0], X[N//2:, 1], c=\"blue\", label=\"class 1\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYyruTDKzttQ"
      },
      "outputs": [],
      "source": [
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ud6eXh23MVyF"
      },
      "source": [
        "## Train some models\n",
        "\n",
        "Let's start from a logistic regression ([sklearn doc](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression)).\n",
        "\n",
        "\n",
        "We will try 2 types of models:\n",
        "- Logistic regression as the linear model\n",
        "- Feed Forward Neural Network as the non-linear model\n",
        "\n",
        "For this lab, since we want to focus on evaluation of model performance, we're going to use a sklearn implementation for the neural network. In the next labs, as we're going to explore more complex architectures, we're going to use Pytorch instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LKgpcppqNpoH"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "#initiate the class\n",
        "logreg = LogisticRegression()\n",
        "#feed it with X and y\n",
        "logreg.fit(X, y)\n",
        "#plot the decision surface\n",
        "plot_decision_surface(model=logreg, x=X, y=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "weLS_eJHKpQ4"
      },
      "source": [
        "Let's try a feed-forward neural net.\n",
        "\n",
        "Sklearn implementation is quite simple and abstracts away most of the complexity of neural networks. You don't need to specify several components that you'd need in a more complex (and complete) framework like Pytorch, like the loss function.\n",
        "\n",
        "Binary cross entropy is a common loss function used in machine learning, particularly for binary classification tasks where the goal is to predict one of two possible outcomes. It measures the difference between the predicted probabilities and the actual binary labels of a given dataset.\n",
        "\n",
        "When training neural networks for binary classification, we take the loss to be the __cross-entropy error function__:\n",
        "\n",
        "$$\n",
        "L({\\boldsymbol \\theta}) =  -\\frac1n \\sum_{i=1}^n \\bigg[y_i  \\log \\hat y_i + (1 - y_i)  \\log (1 - \\hat y_i)\\bigg]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PI31Ut3nQb5I"
      },
      "outputs": [],
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "nn_kwargs= {\"hidden_layer_sizes\":(150,), # number of neurons\n",
        "            \"activation\":\"relu\", # non-linear activation function (through all the network)\n",
        "            \"max_iter\":250, # epochs\n",
        "            \"solver\":\"adam\"} # optimizer\n",
        "\n",
        "custom_nn_kwargs = {} # change this to customize your neural net!\n",
        "ffnn = MLPClassifier(**nn_kwargs)\n",
        "ffnn.fit(X, y)\n",
        "\n",
        "plot_decision_surface(model=ffnn, x=X, y=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Task** : Play around with the hyperparameters of your neural network!\n",
        "\n",
        "Check the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) and modify the `custom_nn_kwargs` dictionary with your preferences."
      ],
      "metadata": {
        "id": "S7lQO5HWc8wv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myKkNJY6AMrM"
      },
      "source": [
        "## Performance assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iAUs0grQS-cx"
      },
      "outputs": [],
      "source": [
        "# Set sizes\n",
        "n = int(N * .8)  # Training points, 80%\n",
        "l = N - n        # Test points, the remaining 20%\n",
        "print(\"num training observations: n=\",  n)\n",
        "print(\"num test observations:     l= \", l)\n",
        "\n",
        "# Data split\n",
        "X_train, y_train = X[:n], y[:n]\n",
        "X_test, y_test = X[n:], y[n:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6eprfe6CUAJR"
      },
      "outputs": [],
      "source": [
        "# Train the two models\n",
        "# logistic regression\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "# neural network\n",
        "ffnn = MLPClassifier(**nn_kwargs)\n",
        "ffnn.fit(X_train, y_train)\n",
        "\n",
        "# Accuracy: rate of correct classifications:\n",
        "# Logistic regression\n",
        "correct_classif = (logreg.predict(X_test) == y_test).astype(int)\n",
        "print(\"LR acc   :\", np.mean(correct_classif))\n",
        "# Neural network\n",
        "y_pred = ffnn.predict(X_test)\n",
        "correct_classif = (y_pred == y_test).astype(int)\n",
        "print(\"NN acc   :\", np.mean(correct_classif))\n",
        "\n",
        "# Plot boundaries\n",
        "plot_decision_surface(model=logreg, x=X_test, y=y_test, title=\"Logistic Regression\")\n",
        "plot_decision_surface(model=ffnn,   x=X_test, y=y_test, title=\"Feed forward Neural Network\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sw7Qh0rRRPgA"
      },
      "source": [
        "#### What's wrong?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTVjSPx0zttR"
      },
      "source": [
        "Let's Look at the confusion matrix.\n",
        "\n",
        "A confusion matrix is a table that summarizes the performance of a classification model by comparing the predicted labels with the actual labels. The confusion matrix shows the number of true positives (TP), true negatives (TN), false positives (FP), and false negatives (FN) for each class.\n",
        "\n",
        "<div style=\"text-align:center;\">\n",
        "    <img src=\"https://assets-global.website-files.com/6266b596eef18c1931f938f9/644aea65cefe35380f198a5a_class_guide_cm08.png\" width=\"40%\">\n",
        "</div>\n",
        "\n",
        "\n",
        "1. True Negative – Indicates how many negative values are predicted as negative only by the model\n",
        "2. False Positive – Indicates how many negative values are predicted as positive values by the model\n",
        "3. False Negative – Indicates how many positive values are predicted as negative values by the model\n",
        "4. True Positive – Indicates how many positive values are predicted as positive only by the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V3zWLEMRzttR"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "\n",
        "# the NN Confusion matrix\n",
        "print(f'accuracy score: {accuracy_score(y_test, y_pred)}')\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fY0HGwt_RMA6"
      },
      "outputs": [],
      "source": [
        "# Plot split data\n",
        "plt.subplot(121)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=np.where(y_train==1, \"blue\", \"red\"))\n",
        "plt.title(\"Training set\")\n",
        "plt.xlim([min(X[:,0])-0.5, max(X[:,0])+0.5])\n",
        "plt.ylim([min(X[:,1])-0.5, max(X[:,1])+0.5])\n",
        "\n",
        "plt.subplot(122)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=np.where(y_test==1, \"blue\", \"red\"))\n",
        "plt.title(\"Test set\")\n",
        "plt.xlim([min(X[:,0])-0.5, max(X[:,0])+0.5])\n",
        "plt.ylim([min(X[:,1])-0.5, max(X[:,1])+0.5])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIYgf6cZeWEo"
      },
      "source": [
        "We did not shuffle the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nbgcu11LehCi"
      },
      "outputs": [],
      "source": [
        "# Shuffle the data!\n",
        "# take some random permutations of N -> randomly permute betweeen 0 and N\n",
        "p = np.random.permutation(N)\n",
        "idx_train = p[:n] # split is still 80/20\n",
        "idx_test = p[n:]\n",
        "\n",
        "# Data split\n",
        "X_train, y_train = X[idx_train], y[idx_train]\n",
        "X_test, y_test = X[idx_test], y[idx_test]\n",
        "\n",
        "# Plot split data\n",
        "plt.subplot(121)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=np.where(y_train==1, \"blue\", \"red\"))\n",
        "plt.title(\"Training set\")\n",
        "plt.subplot(122)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=np.where(y_test==1, \"blue\", \"red\"))\n",
        "plt.title(\"Test set\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zuleJgtVYrk"
      },
      "source": [
        "\n",
        "SkLearn provides many [utilities](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection) to split the data. For example, `train_test_split`, `GroupShuffleSplit` and `StratifiedShuffleSplit`.\n",
        "\n",
        "\n",
        "Try to use the first one, reading [its documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), to split the data. Reminder: we want the test set to be 20% of the data available. Then retrain the models using the training set and assess their performance in terms of accuracy on the test set (you may want to use directly the [appropriate sklearn function](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) to do so).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "... # split the data\n",
        "\n",
        "... # instantiate your logistic regression model\n",
        "\n",
        "... # fit it on the training set\n",
        "\n",
        "... # get the predictions on the test set\n",
        "\n",
        "... # compute the accuracy\n",
        "\n",
        "# do it again for the neural feed-forward neural network"
      ],
      "metadata": {
        "id": "wIqTexYf1suL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Solution:"
      ],
      "metadata": {
        "id": "71FgxeRi1v3k"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3ljaUwnOQ1O"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "idx_train, idx_test = train_test_split(np.arange(N), test_size=0.2, shuffle=True, random_state=42)\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=42)\n",
        "\n",
        "# Data split\n",
        "X_train, y_train = X[idx_train], y[idx_train]\n",
        "X_test, y_test = X[idx_test], y[idx_test]\n",
        "\n",
        "# Plot split data\n",
        "plt.subplot(121)\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=np.where(y_train==1, \"blue\", \"red\"))\n",
        "plt.title(\"Training set\")\n",
        "plt.subplot(122)\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=np.where(y_test==1, \"blue\", \"red\"))\n",
        "plt.title(\"Test set\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OtF6T3xGgukx"
      },
      "outputs": [],
      "source": [
        "# Train the two models\n",
        "#logistic\n",
        "logreg = LogisticRegression()\n",
        "logreg.fit(X_train, y_train)\n",
        "#nn\n",
        "ffnn = MLPClassifier(**nn_kwargs)\n",
        "ffnn.fit(X_train, y_train)\n",
        "y_pred = np.array(ffnn.predict_proba(X_test) < .5, dtype=int)[:, 0]\n",
        "\n",
        "# Evaluate accuracy\n",
        "acc_lr = logreg.score(X_test, y_test)\n",
        "acc_nn = ffnn.score(X_test, y_test)\n",
        "print(\"acc_lr\", acc_lr)\n",
        "print(\"acc_nn\", acc_nn)\n",
        "\n",
        "# Plot boundaries\n",
        "plot_decision_surface(model=logreg, x=X_test, y=y_test, title=\"Logistic Regression\")\n",
        "plot_decision_surface(model=ffnn, x=X_test, y=y_test, title=\"Feed Forward Neural Network\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RF5r7TVpzttS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, confusion_matrix\n",
        "\n",
        "print(f'accuracy score: {accuracy_score(y_test, y_pred)}')\n",
        "cf_mat = confusion_matrix(y_test, y_pred)\n",
        "print('Confusion matrix')\n",
        "print(cf_mat)\n",
        "ConfusionMatrixDisplay.from_predictions(y_test, y_pred)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that accuracy is calculated as:\n",
        "\n",
        "$$accuracy = \\frac{TP+TN}{TP+TN+FP+FN} = \\frac{TP+TN}{N}$$"
      ],
      "metadata": {
        "id": "BpRxZpB069vR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's look at some performance metrics"
      ],
      "metadata": {
        "id": "fF4MT4aC0MjA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's look at the [classification report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html) for our task."
      ],
      "metadata": {
        "id": "4HBzwT3s1qlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "NKVWU91g1yxM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows, for every class, precision, recall, f1-score, support, accuracy, and the macro/weighted averages:\n",
        "\n",
        "$$precision = \\frac{TP}{TP+FP}$$\n",
        "\n",
        "basically, out of all the predicted positives for a class, how many of them are actually positive?\n",
        "\n",
        "$$recall = TPR = \\frac{TP}{TP+FN}$$\n",
        "\n",
        "How many of the true positives was I able to predict appropriately? (also called true positive rate, TPR, or sensitivity)\n",
        "\n",
        "$$F1score = \\frac{2 \\cdot precision \\cdot recall}{precision + recall}$$\n",
        "\n",
        "Macro averages don't consider the support for each class when averaging metrics, weighted averages do.\n",
        "\n",
        "Another metric (not shown in the report is the false positive rate):\n",
        "\n",
        "$$FPR = \\frac{FP}{FP+TN}$$"
      ],
      "metadata": {
        "id": "bP2eUl0Z2FOC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96tSPRKszttS"
      },
      "source": [
        "ROC (Receiver Operating Characteristic) curve is a graphical representation of the performance of a binary classification model.\n",
        "\n",
        "The ROC curve plots the TPR against the FPR at various classification thresholds.\n",
        "\n",
        "The area under the ROC curve (AUC) is a scalar value that represents the performance of the binary classification model.\n",
        "AUC ranges between 0 and 1, where 0 represents a model that makes all predictions wrong, and 1 represents a perfect model that makes all predictions correctly.\n",
        "\n",
        "Interpretation of the ROC curve and AUC:\n",
        "\n",
        "- The closer the ROC curve is to the upper-left corner of the plot (TPR=1, FPR=0), the better the performance of the binary classification model (and the best threshold for your hyperparameter is the one closest to it).\n",
        "- If the ROC curve is a diagonal line, it means that the model performs no better than random chance.\n",
        "- An AUC of 0.5 indicates that the model performs no better than random chance, while an AUC of 1 indicates perfect classification.\n",
        "\n",
        "AUC can be interpreted as the probability that the model will correctly classify a randomly chosen positive instance higher than a randomly chosen negative instance.\n",
        "\n",
        "In summary, the ROC curve and AUC are useful tools for evaluating the performance of binary classification models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nIkVkgY3zttS"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "def plot_roc_curve(true_y, y_prob, name):\n",
        "    \"\"\"\n",
        "    plots the roc curve based of the probabilities\n",
        "    \"\"\"\n",
        "    fpr, tpr, thresholds = roc_curve(true_y, y_prob)\n",
        "    plt.plot(fpr, tpr, label=name)\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "\n",
        "plot_roc_curve(y_test, ffnn.predict_proba(X_test)[:,1], 'FFNN')\n",
        "print(f'FFNN AUC score: {roc_auc_score(y_test, ffnn.predict(X_test) )}')\n",
        "\n",
        "\n",
        "plot_roc_curve(y_test, logreg.predict_proba(X_test)[:,1], 'Logistic Regression')\n",
        "print(f'Logistic regression AUC score: {roc_auc_score(y_test, logreg.predict_proba(X_test)[::,1] )}')\n",
        "\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Given the prediction of one of your models and the true y for the test set, test out some of the functions for the [sklearn metrics module](https://scikit-learn.org/stable/modules/model_evaluation.html)."
      ],
      "metadata": {
        "id": "dhf0PnhdCXZk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = ffnn.predict(X_test)\n",
        "y_proba = ffnn.predict_proba(X_test)[:,1] # notice that we get the column relative to the \"positive\" class\n",
        "\n",
        "from sklearn.metrics import ...\n",
        "\n",
        "# your tests here"
      ],
      "metadata": {
        "id": "S0VgroQwCwCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Solution"
      ],
      "metadata": {
        "id": "f1hAzbkICw6x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = ffnn.predict(X_test)\n",
        "y_proba = ffnn.predict_proba(X_test)[:,1] # notice that we get the column relative to the \"positive\" class\n",
        "\n",
        "from sklearn.metrics import average_precision_score, balanced_accuracy_score, log_loss\n",
        "\n",
        "print(f\"Average precision: {average_precision_score(y_test, y_pred)}\")\n",
        "print(f\"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred)}\")\n",
        "print(f\"Log loss: {log_loss(y_test, y_proba)}\")"
      ],
      "metadata": {
        "id": "ekrV2W7XCy0M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr3mqNKX2y8B"
      },
      "source": [
        "## Can we say which model is the best?\n",
        "\n",
        "What is a T-test?\n",
        "\n",
        "The null hypothesis of a T-test is a statement that there is no significant difference between the means of two groups.\n",
        "\n",
        "T-test is done by building the sample mean and the sample variance for the classification case.\n",
        "\n",
        "There are two types of T-tests:\n",
        "- unpaired samples T-test: used when the two groups being compared are independent of each other\n",
        "- paired samples T-test:  used when the two groups are dependent or related\n",
        "\n",
        "In our case we will be testing on the same test set, so we investigate the application of the paired T-test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMK-joaW8CEU"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "$$\n",
        "e_i =\n",
        "\\begin{cases}\n",
        "1, & \\text{if } y_i =   f(x_i;\\hat \\theta)\\\\\n",
        "0, & \\text{if } y_i \\ne f(x_i;\\hat \\theta)\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$$\\overline e = \\frac{1}{l}\\sum_{i=1}^l e_i ;\\qquad s^2 = \\overline e (1 - \\overline e)$$\n",
        "\n",
        "Under [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem) we compute:\n",
        "\n",
        "$$T = \\frac{\\overline e_{nn} - \\overline e_{lr}}\n",
        "           {\\sqrt{ \\frac{s^2_{nn}}{l} + \\frac{s^2_{lr}}{l}}}$$\n",
        "\n",
        "\n",
        "We want to see if we can fulfill the null hypothesis. We need to check if we are in a 95% confidence interval --> if the results of the statistics is inside the interval (-1.96, 1.96).           "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wtY5rl712zZR"
      },
      "outputs": [],
      "source": [
        "# Logistic Regression part\n",
        "e_lr = (y_test == logreg.predict(X_test)).astype(int)\n",
        "mean_e_lr = e_lr.mean()\n",
        "s2_lr = mean_e_lr * (1 - mean_e_lr)\n",
        "print(\"mean: {} -- s2: {}\".format(mean_e_lr, s2_lr))\n",
        "\n",
        "# Neural Net part\n",
        "y_pred = (ffnn.predict(X_test)).astype(int)\n",
        "e_nn = (y_test == y_pred).astype(int)\n",
        "mean_e_nn = e_nn.mean()\n",
        "s2_nn = mean_e_nn * (1 - mean_e_nn)\n",
        "print(\"mean: {} -- s2: {}\".format(mean_e_nn, s2_nn))\n",
        "\n",
        "# Test statistics\n",
        "T = (mean_e_nn - mean_e_lr)\n",
        "T /= np.sqrt( s2_nn / l + s2_lr / l )\n",
        "print(\"is T={} in 95\\% confidence interval (-1.96, 1.96) ?\".format(T))\n",
        "\n",
        "# t-test\n",
        "from scipy.stats import ttest_ind\n",
        "tt, p_val = ttest_ind(e_lr, e_nn, equal_var=False)\n",
        "print('t-test: T={:.2f}, p-value={:.4f}'.format(tt, p_val))\n",
        "\n",
        "# paired t-test\n",
        "from scipy.stats import ttest_rel\n",
        "tt, p_val = ttest_rel(e_lr, e_nn)\n",
        "print('t-test: T={:.2f}, p-value={:.4f}'.format(tt, p_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aYsXf0KBI5O5"
      },
      "source": [
        "## Did we finish?\n",
        "\n",
        "- The best model was the neural net,\n",
        "- We estimated its performance,\n",
        "- ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b40Y9ecmYmhS"
      },
      "source": [
        "We retrain the best model on the entire dataset.\n",
        "\n",
        "And [save it](https://scikit-learn.org/stable/model_persistence.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aL4xRLUDYsSo"
      },
      "outputs": [],
      "source": [
        "ffnn_final = MLPClassifier(**nn_kwargs)\n",
        "ffnn_final.fit(X, y)\n",
        "\n",
        "from joblib import dump, load\n",
        "dump(ffnn_final, 'my_neural_net.joblib')\n",
        "loaded_model = load('my_neural_net.joblib')\n",
        "\n",
        "# Check they are actually the same\n",
        "print(ffnn_final.score(X, y))\n",
        "print(loaded_model.score(X, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPTt0iNdS9xZ"
      },
      "source": [
        "---\n",
        "\n",
        "\n",
        "## K-fold cross-validation\n",
        "\n",
        "Say we have a single model and we want to identify a confidence interval for its accuracy.\n",
        "\n",
        "### Split the data: cross-validation\n",
        "\n",
        "Cross-validation is a technique used in machine learning to assess the performance of a model. In addition to making sure that we do not have bias in our dataset. It help us to be sure that whatever is been learnt on the training set it is gonna be generalized to the test set.\n",
        "\n",
        "It involves splitting the available data into training and validation sets, training the model on the training set, and then evaluating its performance on the validation set. This process is repeated several times with different splits of the data to obtain a more robust estimate of the model's performance.\n",
        "\n",
        "\n",
        "There are [several types of cross-validation techniques](https://scikit-learn.org/stable/modules/cross_validation.html), including:\n",
        "\n",
        "- k-fold cross-validation: This involves dividing the data into k equally sized folds, training the model k times, and using a different fold as the validation set each time.\n",
        "- Stratified cross-validation: This is used when the data is imbalanced, and ensures that the class distribution is preserved in the training and validation sets.\n",
        "\n",
        "The main idea behind cross-validation is that each observation in our dataset has the opportunity of being tested. In each round, we split the dataset into  k parts: one part is used for validation, and the remaining  k−1 parts are merged into a training subset for model evaluation. The figure below illustrates the process of 5-fold cross-validation:\n",
        "\n",
        "\n",
        "<div style=\"text-align:center;\">\n",
        "    <img src=\"https://www.mltut.com/wp-content/uploads/2020/05/cross-validation.png\" width=\"30%\">\n",
        "</div>\n",
        "\n",
        "$$ Fold:  k_i $$\n",
        "\n",
        "$$ Train:  D_{−k_i}$$\n",
        "\n",
        "$$ Evaluate: {x_i, y_i} \\in D_{k_i}$$\n",
        "\n",
        "$$Performance = \\frac{1}{N}  \\sum_{i=1}^N   {Performance_i} $$\n",
        "\n",
        "\n",
        "where:\n",
        "\n",
        "$D_{-k_i}$ is the training set with the $i$-th fold removed\n",
        "\n",
        "$D_{k_i}$ is the validation set consisting of the $i$-th fold\n",
        "\n",
        "$\\text{Performance}_i$ is the performance metric (e.g. accuracy) on the $i$-th fold\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P01zmRGnlUA1"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Utility to split the data\n",
        "kfcv = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "fold_iterator = kfcv.split(X, y)\n",
        "\n",
        "# Utility to split the data\n",
        "acc_nn = []\n",
        "\n",
        "for idx_train, idx_val in fold_iterator:\n",
        "\n",
        "    # split data\n",
        "    X_train, y_train = X[idx_train], y[idx_train]\n",
        "    X_val, y_val = X[idx_val], y[idx_val]\n",
        "\n",
        "    # train model\n",
        "    ffnn = MLPClassifier(**nn_kwargs)  # Remember: train the model from scratch.\n",
        "    ffnn.fit(X_train, y_train)\n",
        "\n",
        "    # evaluate model\n",
        "    current_acc = ffnn.score(X_val, y_val)\n",
        "    acc_nn.append(current_acc)\n",
        "\n",
        "print(\"Acc list:\", acc_nn)\n",
        "print(\"This is our estimated accuracy:  {:.3f} +- {:.3f}\".format(np.mean(acc_nn), np.std(acc_nn)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R11TJvRzzttS"
      },
      "source": [
        "Here, we will have a set of ten models, but the question is which one to select?\n",
        "\n",
        "We have two additional alternatives:\n",
        "\n",
        "- Use all the data for training:\n",
        "    - build a new model M, this model will be a richer model in term of information, it will be better in in term of probability than the other models simply because we are considering more data.\n",
        "\n",
        "- Ensemble of models:\n",
        "    - We possess k models and we can employ all of them and establish a voting mechanism (A voting mechanism is a technique used in ensemble modeling where multiple models are combined to make a single prediction. Each model in the ensemble is allowed to make its own prediction based on the input data, and then a final prediction is made by taking a vote among the individual predictions.)\n",
        "    - Ensemble models are more resilient since they are less affected by individual data points, but the downside is the computational expense.  \n",
        "<div style=\"text-align:center;\">\n",
        "    <img src=\"https://raw.githubusercontent.com/Project-MONAI/tutorials/9b796e43e527f29c6b8563b573513dff0fd86d98/figures/models_ensemble.png\" width=\"50%\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iz4k0X72uVg1"
      },
      "source": [
        "We could have also compared the two models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0f4-k1e8xG3O"
      },
      "outputs": [],
      "source": [
        "# Utility to split the data\n",
        "kfcv = StratifiedKFold(n_splits=10, shuffle=True)\n",
        "fold_iterator = kfcv.split(X, y)\n",
        "\n",
        "# Utility to split the data\n",
        "acc_nn = []\n",
        "acc_lr = []\n",
        "\n",
        "for idx_train, idx_val in fold_iterator:\n",
        "\n",
        "    X_train, y_train = X[idx_train], y[idx_train]\n",
        "    X_val, y_val = X[idx_val], y[idx_val]\n",
        "\n",
        "    ffnn = MLPClassifier(**nn_kwargs)  # Remember: train the model from scratch.\n",
        "    ffnn.fit(X_train, y_train)\n",
        "\n",
        "    logreg = LogisticRegression()\n",
        "    logreg.fit(X_train, y_train)\n",
        "\n",
        "    current_nn_acc = ffnn.score(X_val, y_val)\n",
        "    acc_nn.append(current_nn_acc)\n",
        "    current_lr_acc = logreg.score(X_val, y_val)\n",
        "    acc_lr.append(current_lr_acc)\n",
        "\n",
        "print(\"LogReg list:   \", acc_lr)\n",
        "print(\"NeuralNet list:\", acc_nn)\n",
        "\n",
        "print(\"LogReg:     {:.3f} +- {:.3f}\".format(np.mean(acc_lr), np.std(acc_lr)))\n",
        "print(\"NeuralNet:  {:.3f} +- {:.3f}\".format(np.mean(acc_nn), np.std(acc_nn)))\n",
        "\n",
        "# Paired two sample test\n",
        "T, p_val = ttest_rel(acc_lr, acc_nn)\n",
        "print('t-test: T={:.2f}, p-value={:.4f}'.format(T, p_val))\n",
        "print(\"is T={:.2f} in 95\\% confidence interval (-1.96, 1.96) ?\".format(T))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XSeLHmXkM6Ih"
      },
      "source": [
        "## More than two models and hyper-parameter tuning\n",
        "\n",
        "In the context of neural networks, parameters refers to a set of weights and biases that are learned during the training process.\n",
        "A hyperparameter, on the other hand, is a configuration variable that is set before the training process begins. These variables affect the behavior of the training algorithm itself, and can have a significant impact on the performance of the resulting model.\n",
        "\n",
        "Examples of hyperparameters include:\n",
        "\n",
        "    - learning rate\n",
        "    - the number of hidden layers\n",
        "    - the number of neurons in each layer\n",
        "    - the activation function used in each layer\n",
        "\n",
        "The performance of a neural network model heavily depends on the choice of hyperparameters: Hyperparameters can be adjusted in order to find the best combination for a given problem. This process is often referred to as hyperparameter tuning, and can be done using various techniques such as [grid search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html), [random search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html), etc..\n",
        "\n",
        "It can be a time-consuming process as it requires training multiple models with different hyperparameter configurations. However, it's a crucial step in building a successful machine learning model as it can significantly improve its performance and generalization ability.\n",
        "\n",
        "<div style=\"text-align:center;\">\n",
        "    <img src=\"https://miro.medium.com/v2/resize:fit:640/format:webp/1*5mStLTnIxsANpOHSwAFJhg.png\" width=\"40%\">\n",
        "</div>\n",
        "\n",
        "\n",
        "We will learn how to apply the grid search hyper-parameter tuning on the number of neurons of the hidden layer and the activation function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zdr8cuYxNbTz"
      },
      "outputs": [],
      "source": [
        "# neurons - activation\n",
        "model_parameters = [(50, \"tanh\"),\n",
        "                    (50, \"relu\"),\n",
        "                    (150, \"tanh\"),\n",
        "                    (150, \"relu\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiBS8ryBzttT"
      },
      "source": [
        "<div style=\"text-align:center;\">\n",
        "    <img src=\"https://studymachinelearning.com/wp-content/uploads/2019/10/summary_activation_fn.png\" width=\"50%\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFr9fuUZZiJL"
      },
      "source": [
        "### Data split\n",
        "\n",
        "The model is trained on the training set and evaluated on the testing set to assess its performance on unseen data.\n",
        "The validation set is used to tune hyperparameters such as the learning rate, and number of hidden units, etc.\n",
        "\n",
        "<div style=\"text-align:center;\">\n",
        "    <img src=\"https://b1739487.smushcdn.com/1739487/wp-content/uploads/2021/04/train-and-test-1-min-1.png?lossy=0&strip=1&webp=1\" width=\"35%\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "exBdNL6WcmDX"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=True)\n",
        "X_atr, X_val, y_atr, y_val = train_test_split(X_train, y_train, test_size=0.25, shuffle=True)\n",
        "\n",
        "# Model selection\n",
        "acc_list = []\n",
        "for (neurons, activation) in model_parameters:\n",
        "    print(\"Training NN with {} neurons and {} activation\".format(neurons, activation))\n",
        "\n",
        "    model = MLPClassifier(hidden_layer_sizes=(neurons,),\n",
        "                          activation=activation,\n",
        "                          max_iter=100,\n",
        "                          solver=\"adam\")\n",
        "    model.fit(X_atr, y_atr)\n",
        "\n",
        "    acc = model.score(X_val, y_val)\n",
        "    acc_list.append(acc)\n",
        "\n",
        "imax = np.argmax(acc_list)\n",
        "print(\"Best model parameters:\", model_parameters[imax])\n",
        "\n",
        "# Performance of best model\n",
        "(neurons, activation) = model_parameters[imax]\n",
        "best_model = MLPClassifier(hidden_layer_sizes=(neurons,),\n",
        "                           activation=activation,\n",
        "                           max_iter=100,\n",
        "                           solver=\"adam\")\n",
        "best_model.fit(X_train, y_train)\n",
        "final_acc = best_model.score(X_test, y_test)\n",
        "print(\"Best model accuracy:\", final_acc)\n",
        "\n",
        "# Final trained model\n",
        "final_model = MLPClassifier(hidden_layer_sizes=(neurons,),\n",
        "                           activation=activation,\n",
        "                           max_iter=100,\n",
        "                           solver=\"adam\")\n",
        "final_model.fit(X, y);\n",
        "dump(final_model, 'my_best_neural_net.joblib')\n",
        "loaded_final_model = load('my_best_neural_net.joblib')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note**: advanced topic for those curious: [nested cross validation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_nested_cross_validation_iris.html)"
      ],
      "metadata": {
        "id": "Y_-DW09dOaTn"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "71FgxeRi1v3k",
        "f1hAzbkICw6x"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}