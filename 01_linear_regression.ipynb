{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatimaEzzedinee/ML-bachelor-course-labs-sp24/blob/main/01_linear_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Machine Learning SP 2023/2024\n",
        "\n",
        "- Prof. Cesare Alippi\n",
        "- Alvise Dei Rossi ([`alvise.dei.rossi@usi.ch`](mailto:alvise.dei.rossi@usi.ch))<br>\n",
        "- Fatima Ezzeddine ([`fatima.ezzeddine@usi.ch`](mailto:fatima.ezzeddine@usi.ch))<br>\n",
        "- Alessandro Manenti ([`alessandro.manenti@usi.ch`](mailto:alessandro.manenti@usi.ch))\n",
        "\n",
        "---\n",
        "\n",
        "# Lab 01: Linear Regression\n",
        "\n",
        "---\n",
        "\n",
        "# A) **Goal**: Predict opossum's length given other body lenghts\n",
        "\n",
        "                                                               .,,.,,*(/*//*(,\n",
        "                                                        *,,**/*,*/***,,*,***/((/***.\n",
        "                                                    /*,,.*,,,***,,,,/*******,,,***/*,*((/*,,\n",
        "                                                 **,,,,*,**,,,,,,*,,,*,***///(//**///(((&&&////,          ..\n",
        "                                              .,,,*,,,,,,*****//****//(((/((////(#(((/((&&@&%,*/#///(///&&&&&#\n",
        "                                           .,*,,,**,,,,,**,*,*(//*/(//(/(((/////////(/(*/@@@%#/#((((#(#&&&&&&*\n",
        "                                          ,*,.,.,,,,*,,,***///((((/((((((///////(*//((//*,&%%,,,,*((***(&&&(\n",
        "                                        .,,,,*,,,,,,,****/*///(*/((####//(((//*((((/(((*......,,,*//*,,,,*,\n",
        "                                       ./(**,,*,,,,,,**////((/(###/###(##/(//**(#%#/((/,,....,,,,*//,,,,,,,.\n",
        "                                      ,(((//***//***,,*//(//(*(/(##%%#(//((#((/(/(##(((*,..,,*,.,*(*,,,**,*,\n",
        "                                   .####((//(/*////*//((#%(#(/(((###%(//((#%(##%##(####/,...*/@/,,*,*(%*,,*\n",
        "                                 /#%###      .,*#%(#/#&&&%%/##(###%##(//(#%@&&%&#%%%%#(/*,...(%(..,,%&&,,,\n",
        "                              //(/*,          .(&%%%#%%%&#%%((((/(#(#(##%&@@@@&@@@&&&&%((//. .,...,***..\n",
        "                 \\          /((//*              /%&%%&&@%######(/((((###((&@@@@@&@@%*   /./(...,.,***,,\n",
        "                  (/(//(/((/                     %%&%%&#(##%%%%%%%%%%###(   &@@@@@*        .#..../**,&&&\n",
        "                                                 ,%#%&@     ,&@@@@,           @&@@@           ..,**,&%%&&,\n",
        "                                                  (@@@@      @@@@@             &&@@&                 &%@@@&\n",
        "                                                   @&&@@,     @@@@@@@ #(        %&&@&*                  *@&%&%#&.\n",
        "\n"
      ],
      "metadata": {
        "collapsed": false,
        "id": "nYErT7xxuoxi"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSzcDPkmJ6qz"
      },
      "source": [
        "# A.1) Collect and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6PrPPELjg1-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd             # Library for data manipulation\n",
        "from sklearn.metrics import mean_squared_error          # Function to calculate the Mean Squared Error (our performance measure)\n",
        "import numpy as np              # Library for arrays\n",
        "import matplotlib.pyplot as plt # Library for plotting\n",
        "\n",
        "\n",
        "url = 'https://drive.switch.ch/index.php/s/JDmUsXBZtvM5m1D/download'    # Data location\n",
        "dataframe = pd.read_csv(url)    # Download the data as a pandas dataframe\n",
        "dataframe.head()                # Show the first 5 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBm7jy-hjg1_"
      },
      "outputs": [],
      "source": [
        "data = dataframe.to_numpy()   # Convert the dataframe to a numpy array\n",
        "print(data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3N3p7SDjjg1_"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split    # Function for splitting dataset into train and test\n",
        "\n",
        "# Split data into train and test\n",
        "train, test = train_test_split(data, train_size=0.7, shuffle=True, random_state=0)\n",
        "print('train_data shape:', train.shape)\n",
        "print('test_data shape:', test.shape)\n",
        "\n",
        "\"\"\"\n",
        "mu = train.mean(axis=0)\n",
        "sigma = train.std(axis=0)\n",
        "train = (train - mu)/sigma\n",
        "test = (test-mu)/sigma\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aXrZ4FvtNCDT"
      },
      "outputs": [],
      "source": [
        "# Divide input data (x) and labels (y)\n",
        "x_train = train[:, 0:6]\n",
        "y_train = train[:, -1]\n",
        "\n",
        "x_test = test[:, 0:6]\n",
        "y_test = test[:, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBJX9OqvOcZu"
      },
      "source": [
        "**Train set** of $n_{train}=72$ observations $\\{(\\mathbf x_1, y_1), (\\mathbf x_2, y_2) ,\\dots,(\\mathbf x_{n_{train}}, y_{n_{train}})\\}$, where $\\mathbf x_i\\in\\mathbb R^{d}$, with $d=6$ and $y_i\\in\\mathbb R$. All the observations are stack to form\n",
        "\n",
        "$$\n",
        "x\\_train = \\left[\n",
        "\\begin{array}{c}\n",
        "\\mathbf x_1\\\\\n",
        "\\mathbf x_2\\\\\n",
        "\\vdots \\\\\n",
        "\\mathbf x_{n_{train}}\n",
        "\\end{array}\n",
        "\\right]\n",
        "\\in \\mathbb{R}^{n_{train}\\times d},\n",
        "\\qquad\n",
        "y\\_train = \\left[\n",
        "\\begin{array}{c}\n",
        "y_1 \\\\\n",
        "y_2 \\\\\n",
        "\\vdots \\\\\n",
        "y_{n_{train}}\n",
        "\\end{array}\n",
        "\\right]\n",
        "\\in \\mathbb{R}^{n_{train}}\n",
        "$$\n",
        "\n",
        "**Test set** of $n_{test}=32$ observations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRFryiYXNCdf"
      },
      "outputs": [],
      "source": [
        "# Let's plot some data\n",
        "plt.scatter(x_train[:, 0], y_train)  # x_train[:, 0] --> original Head_Length\n",
        "plt.xlabel(\"Head length [mm]\")\n",
        "plt.ylabel(\"Total Opossum length [cm]\")\n",
        "plt.grid(alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FWfVU_hXOlu8"
      },
      "source": [
        "# A.2) Our model: Linear prior\n",
        "\n",
        "We assume there is function $f(x)$ that links opossum's head length to its total length:\n",
        "$$\n",
        "y = f(x) + \\eta\n",
        "$$\n",
        "where $\\eta \\sim N(0, \\sigma^2_\\eta)$.\n",
        "\n",
        "We assume that $f(.)$ is linear:\n",
        "\n",
        "$$\n",
        "y = \\theta_0 + \\theta_1 x + \\eta\n",
        "$$\n",
        "\n",
        "where $\\theta_0$ and $\\theta_1$ are the unknown parameters of our model\n",
        "\n",
        "Between all the possible lines (so all the possible combinations of $\\theta_0$ and $\\theta_1$) we want to find the best parameters  ($\\hat{\\theta_0}$, $\\hat{\\theta_1}$) $\\equiv$ $\\hat{\\boldsymbol \\theta}$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9pQjXiWDRkxf"
      },
      "outputs": [],
      "source": [
        "def linear_function(x, theta):\n",
        "    y = theta[0] + x * theta[1]\n",
        "    return y\n",
        "\n",
        "# Plot previous figure\n",
        "plt.scatter(x_train[:, 0], y_train, label='data')\n",
        "plt.xlabel(\"Head length [mm]\")\n",
        "plt.ylabel(\"Total Opossum length [cm]\")\n",
        "plt.grid(alpha=0.5)\n",
        "\n",
        "# Plot also 2 \"random\" guesses\n",
        "equally_spaced_points = np.linspace(start=x_train[:, 0].min(), stop=x_train[:, 0].max(), num=1000)\n",
        "plt.plot(equally_spaced_points, linear_function(equally_spaced_points, theta=[-5, 1]), 'purple', label=r'$\\theta_0: -5$, $\\theta_1: 1$')\n",
        "plt.plot(equally_spaced_points, linear_function(equally_spaced_points, theta=[13, 0.8]), 'orange', label=r'$\\theta_0: 13$, $\\theta_1: 0.8$')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XM0AHt6Rk8T"
      },
      "source": [
        "# A.3) How to find $\\hat{\\boldsymbol \\theta}$: 1D case\n",
        "\n",
        "### The Performance Measure P\n",
        "Since $\\hat{\\boldsymbol \\theta}$ is unknown, we estimate it from the data, by minimising the following performance measure **on the training data**:\n",
        "$$ \\hat{\\boldsymbol \\theta} = \\mathop{\\mathrm{arg\\,min}}_{\\boldsymbol \\theta} \\frac{1}{n} \\sum_{i=1}^n  (y_i - f(x_i;\\boldsymbol \\theta))^2 $$\n",
        "\n",
        "Note that the $x_i$ is the i-th opossum.\n",
        "\n",
        "Finally, to make a new prediction we use the parameters $\\hat{\\boldsymbol \\theta}$\n",
        "$$\\hat y = f\\left(x; \\hat{\\boldsymbol \\theta}\\right).$$\n",
        "\n",
        "### Parameter estimation\n",
        "\n",
        "\n",
        "**Data in compact form:**\n",
        "\n",
        "We can add a \"virtual feature\" to the input to write everything in a compact form. More in detail we can prepend a '1' to each $\\mathbf x$, this way we can write $f(x,\\boldsymbol \\theta)$ in a compact form. In fact:\n",
        "$$\n",
        "f(x,\\boldsymbol \\theta) = x^\\top \\boldsymbol \\theta = \\theta_0 1 + \\theta_1 x\n",
        "$$\n",
        "\n",
        "\n",
        "**Find $\\hat{\\boldsymbol \\theta}$**\n",
        "\n",
        "We showed in class that the best set of parameters $\\hat{\\boldsymbol \\theta}$\n",
        "can be found by solving the linear system\n",
        "$$\n",
        "X^\\top Y - X^\\top X \\boldsymbol \\theta = 0\n",
        "$$\n",
        "with respect to the $\\boldsymbol \\theta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCk1Q-HYjg2E"
      },
      "outputs": [],
      "source": [
        "# Solve the linear system with np.linalg.solve()\n",
        "# Compact form\n",
        "head_lengths = x_train[:,0].reshape(-1,1)            # This is a (72,1) vector\n",
        "ones_vector = np.ones(shape=(len(head_lengths),1))    # Create 72 ones\n",
        "X = np.hstack((ones_vector, head_lengths))         # The compact input\n",
        "print('Input (X) shape:\\t', X.shape)\n",
        "\n",
        "# Find theta_hat\n",
        "theta_hat = np.linalg.solve(a=X.T.dot(X), b=X.T.dot(y_train))   # Solves ax = b with respect to x\n",
        "print('Optimal parameters:\\t', theta_hat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vX6kECiZbzfI"
      },
      "outputs": [],
      "source": [
        "# There are also libraries for this\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# init the model\n",
        "lr = LinearRegression(fit_intercept=False)\n",
        "\n",
        "# estimate parameters\n",
        "lr.fit(X, y_train)\n",
        "theta_hat2 = lr.coef_\n",
        "\n",
        "print('theta_hat = \\t{}'.format(theta_hat))\n",
        "print('theta_hat2 = \\t{}'.format(theta_hat2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7FLmt1kEdCn9"
      },
      "outputs": [],
      "source": [
        "# We can also avoid creating a column of ones\n",
        "lr = LinearRegression(fit_intercept=True)  # default is True\n",
        "lr.fit(x_train[:,0].reshape(-1,1), y_train)\n",
        "theta_hat3 = [lr.intercept_, lr.coef_[0]]\n",
        "print('theta_hat3 = \\t{}'.format(theta_hat3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6iTbKdZAjg2E"
      },
      "outputs": [],
      "source": [
        "# Numerically validate the results\n",
        "train_performance = mean_squared_error(linear_function(x_train[:,0],theta_hat),\n",
        "                                       y_train)\n",
        "test_performance = mean_squared_error(linear_function(x_test[:, 0], theta_hat),\n",
        "                                       y_test)\n",
        "\n",
        "print(\"1D Train performance: \\t\", train_performance)\n",
        "print(\"1D Test performance: \\t\", test_performance)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N53cLrATjg2F"
      },
      "outputs": [],
      "source": [
        "# plot the result\n",
        "plt.scatter(x_train[:,0], y_train, label='Train data')\n",
        "plt.scatter(x_test[:,0], y_test, label='Test data')\n",
        "plt.plot(equally_spaced_points, linear_function(\n",
        "    equally_spaced_points, theta_hat), 'g', label='optimal fit')\n",
        "plt.xlabel(\"Head length [mm]\")\n",
        "plt.ylabel(\"Total Opossum length [cm]\")\n",
        "plt.legend()\n",
        "plt.grid(alpha=0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2nZ-Fm0dm1x"
      },
      "source": [
        "# A.4) How to find $\\hat{\\boldsymbol \\theta}$: d-D case\n",
        "\n",
        "When each input data $\\boldsymbol x_i$ is $d$-dimensional ($\\boldsymbol x_i \\in\\mathbb R^{d}$), the linear model has the same form:\n",
        "$$y = \\mathbf x_i^\\top \\boldsymbol \\theta = \\theta_0 + x_{i1} \\theta_1 + x_{i2} \\theta_2+ \\dots + x_{id}\\theta_d.$$\n",
        "with $\\mathbf\\theta \\in\\mathbb R^{d+1}$.\n",
        "\n",
        "We can prepend a columns of '1's to $\\boldsymbol x$ as before, or use `sklearn.linear_model.LinearRegression`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrnkOkEhg9CE"
      },
      "outputs": [],
      "source": [
        "# Use LinearRegression with all the available features\n",
        "d_linear_model = LinearRegression()\n",
        "d_linear_model.fit(x_train, y_train)\n",
        "\n",
        "# Numerically validate the results\n",
        "# d-dimensional train\n",
        "d_train_pred = d_linear_model.predict(x_train)\n",
        "d_train_performance = mean_squared_error(d_train_pred, y_train)\n",
        "\n",
        "# d-dimensional test\n",
        "d_test_pred = d_linear_model.predict(x_test)\n",
        "d_test_performance = mean_squared_error(d_test_pred, y_test)\n",
        "\n",
        "print(f\"d-D Train performance: \\t{d_train_performance:.2f}\")\n",
        "print(f\"d-D Test performance: \\t{d_test_performance:.2f}\")\n",
        "print(f\"1D Train performance: \\t{train_performance:.2f}\")\n",
        "print(f\"1D Test performance: \\t{test_performance:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZjtWRpcjg2F"
      },
      "source": [
        "# A.5) Liner regression with gradient descent\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv12xP-dj7n7"
      },
      "source": [
        "During the lectures we have seen that, given a model $f(x; {\\boldsymbol \\theta})$ it is possible minimize the training error iteratively using gradient descent:\n",
        "\n",
        "\n",
        "$${\\boldsymbol \\theta}^{i+1} \\gets {\\boldsymbol \\theta}^i - \\varepsilon_L \\frac{\\partial V_n({\\boldsymbol \\theta})}{\\partial {\\boldsymbol \\theta}} \\bigg \\rvert_{{\\boldsymbol \\theta} = {\\boldsymbol \\theta}^i}$$\n",
        "\n",
        "Consider the mean squared error,\n",
        "$$V_n({\\boldsymbol \\theta}) = {1 \\over n}\\sum_{i=1}^n\\left(y_i - f(x_i; \\boldsymbol \\theta)\\right)^2$$\n",
        "In our case $f(x_i; \\boldsymbol \\theta)$ is linear: $f(x_i; \\boldsymbol \\theta) = \\theta_0 + x_{i1} \\theta_1 + x_{i2} \\theta_2+ \\dots + x_{id}\\theta_d.$\n",
        "\n",
        "So, we can write the gradient as:\n",
        "\n",
        "$$\\frac{\\partial V_n({\\boldsymbol \\theta})}{\\partial {\\boldsymbol \\theta}} =\n",
        "\\left[\n",
        "\\begin{array}{c}\n",
        "\\frac{\\partial V_n({\\boldsymbol \\theta})}{\\partial {\\theta_0}} \\\\\n",
        "\\frac{\\partial V_n({\\boldsymbol \\theta})}{\\partial {\\theta_1}} \\\\\n",
        "\\vdots \\\\\n",
        "\\frac{\\partial V_n({\\boldsymbol \\theta})}{\\partial {\\theta_d}}\n",
        "\\end{array}\n",
        "\\right] =\n",
        "\\left[\n",
        "\\begin{array}{c}\n",
        "-{2 \\over n}\\sum_{i=1}^n\\left(y_i - f(x_i; \\boldsymbol \\theta)\\right) \\\\\n",
        "-{2 \\over n}\\sum_{i=1}^n\\left(y_i - f(x_i; \\boldsymbol \\theta)\\right)x_{i1} \\\\\n",
        "\\vdots \\\\\n",
        "-{2 \\over n}\\sum_{i=1}^n\\left(y_i - f(x_i; \\boldsymbol \\theta)\\right)x_{id}\n",
        "\\end{array}\n",
        "\\right] = -{2 \\over n}X^T(Y - X\\boldsymbol \\theta)$$\n",
        "\n",
        "Let's do it in numpy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3qwMDohRx6M"
      },
      "outputs": [],
      "source": [
        "# Compact form\n",
        "ones_vector = np.ones(shape=(x_train.shape[0], 1))    # Create 72 ones\n",
        "x_compact = np.hstack((ones_vector, x_train))         # The compact input\n",
        "\n",
        "def calculate_gradient(X, Y, th):\n",
        "  n = X.shape[0]\n",
        "  gradient = (- 2 / n) * np.dot(X.T, (Y.reshape(-1,1) - np.dot(X, th).reshape(-1,1)))\n",
        "  return gradient\n",
        "\n",
        "def V(X, Y, theta):\n",
        "  return np.mean((Y.reshape(-1,1) - np.dot(X, theta))**2)\n",
        "\n",
        "np.random.seed(0)\n",
        "theta = np.random.uniform(size=(x_compact.shape[1],1))     # initial value theta, it is random you can change it\n",
        "\n",
        "eps = 0.00002                                              # step size (hyperparameter)\n",
        "# eps0 = 0.1\n",
        "steps = 1000                                                # number of GD steps\n",
        "\n",
        "thetas_history = np.zeros(shape=(theta.shape[0], steps+1))  # save the theta values along training\n",
        "loss_history = np.zeros(shape=(steps+1,1))                  # save the error values along training\n",
        "thetas_history[:,0:1] = theta.copy()\n",
        "loss_history[0] = V(x_compact, y_train, theta)\n",
        "\n",
        "for i in range(1, steps+1):\n",
        "  grad = calculate_gradient(x_compact, y_train, theta)\n",
        "  theta = theta - eps * grad\n",
        "  # mask = np.zeros((7,1))\n",
        "  # mask[0] = 1\n",
        "  # theta = theta - eps0 * grad * mask\n",
        "  # log theta and loss\n",
        "  thetas_history[:,i:i+1] = theta.copy()\n",
        "  loss_history[i] = V(x_compact, y_train, theta)\n",
        "\n",
        "\n",
        "thetas_history = np.array(thetas_history)\n",
        "loss_history = np.array(loss_history)\n",
        "print('Optimal theta:', theta.T)\n",
        "print(f'Mean Squared Error: {V(x_compact, y_train, theta):.2f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DsPYR5pRTPyc"
      },
      "outputs": [],
      "source": [
        "thetas_history01 = thetas_history[0:2, :]\n",
        "\n",
        "# code for plotting\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "# Define plot range\n",
        "r0_min, r0_max = thetas_history[0, :].min(), thetas_history[0, :].max()\n",
        "r1_min, r1_max = thetas_history[1, :].min(), thetas_history[1, :].max()\n",
        "x_range = np.linspace(r0_min-0.5, r0_max+0.5, 100)\n",
        "y_range = np.linspace(r1_min-0.5, r1_max+0.5, 100)\n",
        "\n",
        "# Calculate potential in that range\n",
        "theta_0, theta_1 = np.meshgrid(x_range, y_range)\n",
        "zs = np.zeros((100, 100))\n",
        "for i in range(100):\n",
        "    for j in range(100):\n",
        "        T = np.hstack((theta_0[i, j], theta_1[i, j], theta[2:].reshape(-1,))) # NOTICE\n",
        "        zs[i,j] = V(x_compact, y_train, T.reshape(7,1))\n",
        "\n",
        "# Code for plotting\n",
        "fig = plt.figure(figsize=(12,5))\n",
        "ax1 = fig.add_subplot(122)\n",
        "ax1.set_xlabel(r'$\\theta_0$')\n",
        "ax1.set_ylabel(r'$\\theta_1$')\n",
        "c = ax1.contour(theta_0, theta_1, zs, levels=20,  cmap='viridis')\n",
        "plt.clabel(c, inline=1, fontsize=10)\n",
        "\n",
        "ax1.plot(thetas_history01[0, :], thetas_history01[1, :], '-x', color='red')\n",
        "ax1.grid(alpha=0.5)\n",
        "\n",
        "ax2 = fig.add_subplot(121, projection='3d')\n",
        "ax2.plot_trisurf(theta_0.flatten(), theta_1.flatten(), zs.flatten(), cmap='viridis')\n",
        "ax2.set_xlabel(r'$\\theta_0$')\n",
        "ax2.set_ylabel(r'$\\theta_1$')\n",
        "ax2.set_zlabel(r'$V(\\theta)$')\n",
        "ax2.plot(thetas_history01[0, :], thetas_history01[1, :], loss_history.flatten(), '-x', color='red', alpha=1.)\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ew3xhxEMjg2I"
      },
      "source": [
        "**BE CAREFUL!** The hypersurface you see is calculated fixing some values of $\\theta$. This is why it is different from the red path.\n",
        "\n",
        "This is the reason why the red path seems **NOT** to follow the gradient while descending"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvAmK69ukSBH"
      },
      "source": [
        "You can try to play with the step size. What happens if you increase/decrease it?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Is there a problem?"
      ],
      "metadata": {
        "id": "1WFaw1DZls3h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://scontent-zrh1-1.xx.fbcdn.net/v/t1.6435-9/136074878_892971658171606_7021502333096303351_n.jpg?_nc_cat=105&ccb=1-7&_nc_sid=5f2048&_nc_ohc=CQ-hO3U66bQAX-TpDrQ&_nc_ht=scontent-zrh1-1.xx&oh=00_AfC5x-JSlSKQy9vfAofmZZccESAOTjOz79oB3VCTjEBNGQ&oe=662A6CBB\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "PGufn0DUlQw4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYBhDMHDzJYy"
      },
      "source": [
        "# A.6) Confidence intervals for the parameters\n",
        "Assume that $X^\\top X$ is invertible, then defining $X^+$ $\\equiv$ $(X^TX)^{-1}X^T$ we have:\n",
        "\n",
        "$$\n",
        "\\hat \\theta = X^+Y \\sim N\\big(\\theta, \\sigma_\\eta^2 (X^\\top X)^{-1}\\big)\n",
        "$$\n",
        "\n",
        "$$\n",
        "E[\\hat \\theta] = E[X^+Y] = X^+E[Y] = X^+ X\\theta^o = (X^\\top X)^{-1}X^\\top X \\theta^o = \\theta^o \\longrightarrow \\text{Unbiased Estimator}\n",
        "$$\n",
        "\n",
        "$$\n",
        "Var[\\hat \\theta] = Var[X^+Y] = X^+Var[Y] (X^+)^\\top = \\sigma_\\eta^2 (X^\\top X)^{-1} X^\\top X (X^\\top X)^{-1} = \\sigma_\\eta^2 (X^\\top X)^{-1}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\hat{\\sigma_\\eta}^2 = \\frac{1}{n-d} \\sum_{i=1}^n(y_i - f(x_i; \\boldsymbol \\theta))^2\n",
        "$$\n",
        "\n",
        "A rule of thumb is the following\n",
        "\n",
        "* Extract the diagonal from $\\hat{\\sigma_\\eta}^2 (X^\\top X)^{-1}$, which gives you an idea of the variance of each component of $\\theta$.\n",
        "* For each component $\\theta_i$, check if the interval $(\\theta_i - 2\\sigma_i, \\theta_i + 2\\sigma_i)$ contains the zero; if that is the case, we are not very confident that the $\\theta_i\\ne 0$, thus that $x_i$ is relevant in the model.\n",
        "* **BE CAREFUL:** This is a rule of thumb, be sure |$Var[\\hat \\theta]_{ii}$| >> |$Var[\\hat \\theta]_{ij}$| $\\forall$ j\n",
        "* **BE CAREFUL:** Features must be centered around 0."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OIsADfQ4jg2I"
      },
      "outputs": [],
      "source": [
        "sigma_square = d_train_performance / (x_compact.shape[0] - x_compact.shape[1])\n",
        "theta_var =  np.linalg.inv(np.dot(x_compact.T, x_compact)) * sigma_square"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AuDfj8Uqjg2I"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "ticklabels = dataframe.keys()[:-1].insert(0, 'Bias')\n",
        "\n",
        "fig = sns.heatmap(np.log10(np.abs(theta_var)), cbar_kws={'label':r'$log_{10}$(|v|)'},  # Notice we calculated the log of the absolute of theta_var\n",
        "                  xticklabels=ticklabels, yticklabels=ticklabels)\n",
        "fig.set_title(r'Processed covariance matrix of $\\hat{\\theta}$')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZCQGRdruoxx"
      },
      "outputs": [],
      "source": [
        "print(f'theta_hat[0]: {d_linear_model.intercept_:.7f}')\n",
        "print(f'Var[theta_hat[0]]: {theta_var[0,0]:.7f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLpfbZcGuoxx"
      },
      "source": [
        "So, can we set $\\theta_0$ = 0?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3_VCSy3Muoxx"
      },
      "outputs": [],
      "source": [
        "# Use LinearRegression with all the available features\n",
        "nobias_linear_model = LinearRegression(fit_intercept=False)\n",
        "nobias_linear_model.fit(x_train, y_train)\n",
        "\n",
        "# Numerically validate the results\n",
        "# test\n",
        "nobias_test_pred = d_linear_model.predict(x_test)\n",
        "nobias_test_performance = mean_squared_error(nobias_test_pred, y_test)\n",
        "\n",
        "print(f\"d-D Test performance: \\t{d_test_performance:.2f}\")\n",
        "print(f\"d-1 D Test performance: \\t{nobias_test_performance:.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXOYwKgMdnCZ"
      },
      "source": [
        "---\n",
        "\n",
        "## B) **Goal**: apply Regularization to polynomial features\n",
        "\n",
        "Regularization can be used with on the examples seen so far, but in order to appreciate it better, we are going to apply it to polynomials\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cN5W-BU6oUZT"
      },
      "source": [
        "\n",
        "### Example: Polynomials\n",
        "\n",
        "$$f(x;\\boldsymbol \\theta) = \\theta_0 + x \\theta_1 + x^2 \\theta_2 + \\dots + x^d \\theta_d$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKYw9sXpmtGn"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "def polynomial_function(x):\n",
        "    return -1 -x - .1 * x**2 + .5*x**3\n",
        "\n",
        "# generate data\n",
        "n = 40\n",
        "X = np.linspace(-1, 2, n).reshape(n,1)\n",
        "interval = np.linspace(-1, 2, 1000).reshape(1000,1)\n",
        "sigma = 0.2\n",
        "np.random.seed(0)\n",
        "eta = np.random.normal(loc=0, scale=sigma, size=(n,1)) # Add noise\n",
        "Y = polynomial_function(X) + eta\n",
        "\n",
        "# Fit a LOW degree polynomial (line)\n",
        "degree = 1\n",
        "pol_feat = PolynomialFeatures(degree=degree, include_bias=False) # Generate new features x --> x\n",
        "Xpol = pol_feat.fit_transform(X)\n",
        "\n",
        "lr = LinearRegression(fit_intercept=True)\n",
        "lr.fit(Xpol, Y)\n",
        "Y_est = lr.predict(pol_feat.transform(interval))\n",
        "\n",
        "# Fit a HIGH degree polynomial\n",
        "degree = 30\n",
        "\n",
        "pol_feat_high = PolynomialFeatures(degree=degree, include_bias=False) # Generate new features x --> [x^1, x^2, x^3, ..., x^30]\n",
        "Xpol_high = pol_feat_high.fit_transform(X)\n",
        "\n",
        "lr_high = LinearRegression(fit_intercept=True)\n",
        "lr_high.fit(Xpol_high, Y)\n",
        "Y_est_high = lr_high.predict(pol_feat_high.transform(interval))\n",
        "\n",
        "# plot results\n",
        "plt.plot(interval, polynomial_function(interval), label='true fun')\n",
        "plt.scatter(X, Y, label='noisy data', c='k', marker='.', alpha=0.5)\n",
        "plt.plot(interval, Y_est, label='linear fit')\n",
        "plt.plot(interval, Y_est_high, 'r', label='high degree fit')\n",
        "plt.ylim((-2.5, 1))\n",
        "plt.legend()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLuQeYuNn5hc"
      },
      "source": [
        "# B.1) Ridge regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQuCKsazC3KC"
      },
      "outputs": [],
      "source": [
        "# Fit a HIGH degree polynomial\n",
        "degree = 30\n",
        "\n",
        "pol_feat_high = PolynomialFeatures(degree=degree, include_bias=False)\n",
        "Xpol_high = pol_feat_high.fit_transform(X)\n",
        "\n",
        "# Ridge regression\n",
        "from sklearn.linear_model import Ridge\n",
        "ridge_high = Ridge(alpha=1, fit_intercept=True)\n",
        "ridge_high.fit(Xpol_high, Y)\n",
        "Y_ridge = ridge_high.predict(pol_feat_high.transform(interval))\n",
        "\n",
        "# plot results\n",
        "plt.plot(interval, polynomial_function(interval), label='true fun')\n",
        "plt.scatter(X, Y, label='noisy data', c='k', marker='.', alpha=0.5)\n",
        "plt.plot(interval, Y_est, label='linear fit')\n",
        "plt.plot(interval, Y_ridge, 'r', label='high degree fit with ridge')\n",
        "plt.ylim((-2, 0.8))\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wdCLz1O2uoxz"
      },
      "source": [
        "Why do you think some points are overfitting more?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zG0OruNeuox0"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "Xpol_high_scaled = scaler.fit_transform(Xpol_high)\n",
        "\n",
        "# Ridge regression\n",
        "ridge_high = Ridge(alpha=0.01, fit_intercept=True)\n",
        "ridge_high.fit(Xpol_high_scaled, Y)\n",
        "Y_ridge = ridge_high.predict(scaler.transform(pol_feat_high.transform(interval)))\n",
        "\n",
        "# plot results\n",
        "plt.plot(interval, polynomial_function(interval), label='true fun')\n",
        "plt.scatter(X, Y, label='noisy data', c='k', marker='.', alpha=0.5)\n",
        "plt.plot(interval, Y_est, label='linear fit')\n",
        "plt.plot(interval, Y_ridge, 'r', label='high degree fit with ridge')\n",
        "plt.ylim((-2, 0.8))\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tOzMuL30uox0"
      },
      "source": [
        "**BE CAREFUL:** At the begining of this class we did not normalize the Opossum's features. As you may have noticed it is often either a good practice or essential!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1lQ_Uy_Anlyo"
      },
      "source": [
        "# B.2) Lasso regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi6ksrt9jg2J"
      },
      "source": [
        "Try implementing Lasso regression. As input data use the same data as before, again with degree=30 (So you may re-use `Xpol_high_scaled`, `Y`, ...)\n",
        "\n",
        "HINT: use `sklearn.linear_model.Lasso`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BNyYsMyvuox1"
      },
      "outputs": [],
      "source": [
        "# Try it yourself!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QsLhpWcrjg2K"
      },
      "source": [
        "Q1) What is approximately the best value $\\hat{\\alpha}$ for $\\alpha$?\n",
        "\n",
        "Q2) What happens for $\\alpha$ << $\\hat{\\alpha}$? Why? (Notice if a warning appears! Why do you think there was no warning with Ridge?)\n",
        "\n",
        "Q3) What happens for $\\alpha$ >> $\\hat{\\alpha}$? Why?"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "1WFaw1DZls3h"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}