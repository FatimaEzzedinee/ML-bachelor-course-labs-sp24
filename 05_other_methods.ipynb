{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatimaEzzedinee/ML-bachelor-course-labs-sp24/blob/main/06_other_methods.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu4HJeN686y2"
      },
      "source": [
        "# Machine Learning SP 2023/2024\n",
        "\n",
        "- Prof. Cesare Alippi\n",
        "- Alvise Dei Rossi ([`alvise.dei.rossi@usi.ch`](mailto:alvise.dei.rossi@usi.ch))<br>\n",
        "- Fatima Ezzeddine ([`fatima.ezzeddine@usi.ch`](mailto:fatima.ezzeddine@usi.ch))<br>\n",
        "- Alessandro Manenti ([`alessandro.manenti@usi.ch`](mailto:alessandro.manenti@usi.ch))\n",
        "\n",
        "---\n",
        "\n",
        "# Lab 06: Other methods\n",
        "\n",
        "In this lab we will see how to use some of the more advanced methods that we saw in the last lectures. In particular, we will focus on their use for very simple (and very famous) classification tasks such as the XOR problem and the Iris dataset.\n",
        "\n",
        "Let's start by importing what's needed and defining a helper function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IgAvdbC1482"
      },
      "outputs": [],
      "source": [
        "# first we define some helper functions to generate data and plot results\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import ListedColormap\n",
        "plt.rcParams[\"image.cmap\"] = \"PiYG_r\"  # Define the colors to use in the plot\n",
        "plt.rcParams[\"scatter.edgecolors\"] = 'k'\n",
        "\n",
        "# function to plot decision boundaries\n",
        "def plot_decision_surface(model, x, y, transform=lambda x:x, plot_margins=False):\n",
        "  #init figure\n",
        "  fig = plt.figure()\n",
        "\n",
        "  # Create mesh\n",
        "  h = .01  # step size in the mesh\n",
        "  x_min, x_max = x[:, 0].min() - .5, x[:, 0].max() + .5\n",
        "  y_min, y_max = x[:, 1].min() - .5, x[:, 1].max() + .5\n",
        "  xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                        np.arange(y_min, y_max, h))\n",
        "\n",
        "  # plot train data (assuming 2 dimensions for the task)\n",
        "  plt.scatter(x[:, 0], x[:, 1], c=y)\n",
        "\n",
        "  plt.xlim(xx.min(), xx.max())\n",
        "  plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "  plt.xlabel(r'$x_1$')\n",
        "  plt.ylabel(r'$x_2$')\n",
        "\n",
        "  y_pred = model.predict(transform(np.c_[xx.ravel(), yy.ravel()]))\n",
        "\n",
        "  y_pred = y_pred.reshape(xx.shape)\n",
        "  plt.contourf(xx, yy, y_pred, alpha=.2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fle838Qjy4I9"
      },
      "source": [
        "# A) Support Vector Machines\n",
        "\n",
        "SVMs are powerful ML models, capable of performing linear or nonlinear classification, regression and more. They are well suited for most tabular small-to-medium sized tasks.\n",
        "\n",
        "Let's see how we can use Support Vector Machines and the kernel trick to solve classification problems where linear approaches fail.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pYRAMU22KO6C"
      },
      "source": [
        "## A.1) The XOR problem\n",
        "\n",
        "As already discussed, linear methods are unable to solve problems in which the classes are not linearly separable.\n",
        "\n",
        "The XOR problem is the classic example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rlSbxu8_jM5"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "x = np.random.randn(200, 2) # sample some points from a bivariate gaussian\n",
        "y = np.logical_xor(x[:, 0] > 0., x[:, 1] > 0.)\n",
        "\n",
        "x[x > 0.] += .5 # this is just to explicitely separate them\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(x[:, 0], x[:, 1], c=y)\n",
        "plt.xlabel(r'$x_0$')\n",
        "plt.ylabel(r'$x_1$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Remember, a **kernel** (oversimplifying a lot) is a function that gives us a particular measure of affinity between two points.\n",
        "The idea is to **embed** the input data into a new representation; if such new representation is effective, the task is more easily solvable in the embedding space."
      ],
      "metadata": {
        "id": "TCVCLpEQ27i0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Linear kernel"
      ],
      "metadata": {
        "id": "ZdytzRlU3B7K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the **linear kernel** this is the dot product of the input samples:\n",
        "\n",
        "$$K(x_1, x_2) = x_1^Tx_2$$\n",
        "\n",
        "It is then applied to any combination of two data points (samples) in the dataset.\n",
        "\n",
        "Training a [**SVC**](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html) (Support Vector Classifier) on a linear kernel results in an untransformed feature space, where the hyperplane and the margins are straight lines."
      ],
      "metadata": {
        "id": "PYizPxuQrQ4b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fhzsyXIi94NU"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "def k(x1, x2):\n",
        "  return np.dot(x1, x2.T)\n",
        "# Note: different notation of the dot product here is because the X matrix is given as a nxd matrix\n",
        "# and observations are treated as row vectors\n",
        "\n",
        "svm = SVC(kernel=k)\n",
        "\n",
        "svm.fit(x, y)\n",
        "\n",
        "plot_decision_surface(svm, x, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7PyRPPTXzVW"
      },
      "source": [
        "### Polynomial kernel\n",
        "\n",
        "As you can see using a linear kernel we are still limited to drawing hyperplanes, let's go back and try out something more interesting...\n",
        "\n",
        "We can use kernels in the dual formulation of the SVM problem to project the input space in a high (possibly infinite) dimensional space. This allows to deal with nonlinear classification tasks. Let's see some cases.\n",
        "\n",
        "The **polynomial kernel** changes the notion of similarity. The kernel function is defined as:\n",
        "\n",
        "$$K(x_1, x_2) = (\\gamma \\cdot x_1^T x_2 + r)^d$$\n",
        "\n",
        "where $d$ is the degree of the polynomial, $\\gamma$ controls the influence of each individual training sample on the decision boundary and $r$ is the bias term (coef0).\n",
        "\n",
        "**Note:** Using a polynomial kernel is equivalent to creating [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) and then fitting a SVC with a linear kernel on the transformed data, although this alternative approach would be computationally expensive for most datasets."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import euclidean_distances\n",
        "\n",
        "def k(x1, x2):\n",
        "   gamma = 0.25\n",
        "   r = 1\n",
        "   d = 3\n",
        "   return  (gamma * np.dot(x1, x2.T) + r)**d\n",
        "\n",
        "svm = SVC(kernel=k)\n",
        "\n",
        "svm.fit(x, y)\n",
        "\n",
        "plot_decision_surface(svm, x, y)"
      ],
      "metadata": {
        "id": "Dq7TBilHrt26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RBF kernel\n",
        "\n",
        "Now let's check the radial basis function (RBF) kernel, also known as the Gaussian kernel, is defined as:\n",
        "\n",
        "$$K(x_1, x_2) = exp(-\\gamma \\cdot \\lVert x_1 - x_2 \\rVert ^2)$$\n",
        "\n",
        "The larger the euclidean distance between two points $\\lVert x_1 - x_2 \\rVert$, the closer the kernel function is to zero. In fact this is a bell-shaped function varying from 0 to 1, decreasing exponentially with the distance of the two points. This means that two points far away are more likely to be dissimilar."
      ],
      "metadata": {
        "id": "rtK-TaJ11Kyp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def k(x1, x2):\n",
        "   gamma = 0.25\n",
        "   return  np.exp(-gamma * euclidean_distances(x1, x2)**2)\n",
        "\n",
        "svm = SVC(kernel=k)\n",
        "\n",
        "svm.fit(x, y)\n",
        "\n",
        "plot_decision_surface(svm, x, y)"
      ],
      "metadata": {
        "id": "d52TViVp4KZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sklearn kernel implementations"
      ],
      "metadata": {
        "id": "vo__ZnfE4vQ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course **we do not need to define kernels functions by hand**, they are already implemented in `scikit-learn`. Sklearn implementations take advantage of the kernel trick and are much faster than actually calculating for every pair of instances the similarity measure.\n",
        "\n",
        "We can also check which are the support vectors.\n",
        "\n"
      ],
      "metadata": {
        "id": "8SJh9lchsGZk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Exercise:**\n",
        "\n",
        "In the next cell, check the influence of the hyperparameters on the decision boundary."
      ],
      "metadata": {
        "id": "2Wp5dx029RT8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiAuV7z0YvuM"
      },
      "outputs": [],
      "source": [
        "# Your choices for the hyperparameters:\n",
        "kernel_choice = \"poly\" # common choices here are \"poly\", \"rbf\", \"sigmoid\"\n",
        "C = 1 # Regularization term, the strength of the regularization is inversely proportional to it. Must be strictly positive.\n",
        "degree = 3 # this is relevant only if kernel_choice is set to \"poly\"; otherwise it's ignored.\n",
        "gamma = 0.25 # relevant when kernel_choice is \"poly\" or \"rbf\" or \"sigmoid\"\n",
        "coef0 = 1 # bias term, relevant when kernel choice is \"poly\" or \"sigmoid\"\n",
        "\n",
        "### Model fitting\n",
        "svm = SVC(kernel=kernel_choice,\n",
        "          C=C,\n",
        "          degree=degree,\n",
        "          coef0=coef0,\n",
        "          gamma=.2)\n",
        "svm.fit(x, y)\n",
        "\n",
        "plot_decision_surface(svm, x, y)\n",
        "# If you want to see the support vectors uncomment below\n",
        "#plt.scatter(svm.support_vectors_[:, 0], svm.support_vectors_[:, 1], marker='x')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAEiFeQmuPEY"
      },
      "source": [
        "You can find more on the mathematical formulation of scikit-learn's SVC and much more [here](https://scikit-learn.org/stable/modules/svm.html#svc)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e98odkQYXQY"
      },
      "source": [
        "## A.2) Iris dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3X6JtuoMlqCq"
      },
      "source": [
        "As a second case study, for our next classifiers, we are going to use the infamous [Iris](https://archive.ics.uci.edu/ml/datasets/iris) dataset, where the objective is to classify flowers based on some features:\n",
        "\n",
        "1. sepal length in cm\n",
        "2. sepal width in cm\n",
        "3. petal length in cm\n",
        "4. petal width in cm\n",
        "\n",
        "<img src=\"https://media.licdn.com/dms/image/D4D12AQF5vivFTAdZjQ/article-cover_image-shrink_600_2000/0/1700911428185?e=2147483647&v=beta&t=RaJufpE5-ZMvIMZFVTy4dNtvnKHVgmThtTORx-_qu6Q\" alt=\"Iris dataset\" width=\"725\" height=\"375\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nn7xqfdkmfCE"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "iris = load_iris()\n",
        "\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "\n",
        "x.shape, y.shape, set(y), iris.target_names # 150 observations, 4 features, 3 possible classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OvLg7L-imfjV"
      },
      "source": [
        "Let's give a look at the data.\n",
        "\n",
        "We only use two features to make visualization easier, however keep in mind that the inclusion of the other features could help for the classification task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SjvNW28BxGR"
      },
      "outputs": [],
      "source": [
        "features = [\"sepal length in cm\" ,\"sepal width in cm\", \"petal length in cm\", \"petal width in cm\"]\n",
        "features_used = [2,3]\n",
        "x_prime = x[:, features_used]\n",
        "\n",
        "_, ax = plt.subplots()\n",
        "scatter  = ax.scatter(x_prime[:, 0], x_prime[:, 1], c=y)\n",
        "ax.set_xlabel(features[features_used[0]])\n",
        "ax.set_ylabel(features[features_used[1]])\n",
        "ax.legend(\n",
        "    scatter.legend_elements()[0], iris.target_names, loc=\"lower right\", title=\"Classes\"\n",
        ")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try first with SVM"
      ],
      "metadata": {
        "id": "vzh2C_uY7_6d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi0MniB74Q7a"
      },
      "outputs": [],
      "source": [
        "classifier = SVC(kernel='rbf', gamma=1, C=1)\n",
        "\n",
        "classifier.fit(x_prime, y)\n",
        "\n",
        "plot_decision_surface(classifier, x_prime, y)\n",
        "#plt.scatter(classifier.support_vectors_[:, 0], classifier.support_vectors_[:, 1], marker='x')\n",
        "plt.xlabel('petal length (cm)')\n",
        "plt.ylabel('petal width (cm)')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVwVoxFoEi1F"
      },
      "source": [
        "Tuning correctly the hyperparameters is fundamental (check [here](https://scikit-learn.org/stable/auto_examples/svm/plot_rbf_parameters.html)).\n",
        "\n",
        "In particular (for the `rbf` kernel):\n",
        "\n",
        "* `C` : cost of a misclassification error\n",
        "* `gamma`: $1 / \\sigma$ of the Gaussian kernel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W4Tbo-OmeBCR"
      },
      "source": [
        "# B) Decision Trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbIvjOQ49Udr"
      },
      "source": [
        "Again we can use `scikitlearn` to build Decision Trees pretty easily in python.\n",
        "\n",
        "Decision Trees are a non-parametric supervised learning method used for classification and regression.\n",
        "\n",
        "The goal is to create a model that predicts the value of a target variable by learning simple decision rules inferred from the data features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayq8nv8xojjG"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "classifier = DecisionTreeClassifier(max_depth=2,\n",
        "                                    random_state=42)\n",
        "# try also with max_depth 3\n",
        "classifier.fit(x_prime, y)\n",
        "\n",
        "plot_decision_surface(classifier, x_prime, y)\n",
        "plt.xlabel(features[features_used[0]])\n",
        "plt.ylabel(features[features_used[1]])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "reMC63iSnWPJ"
      },
      "source": [
        "Now let's try to build a tree using all the features.\n",
        "\n",
        "We can't anymore look at only two dimensions to understand how the model is operating. Instead we'll look directly at the tree.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p7hQaTYdqx5T"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import plot_tree\n",
        "\n",
        "classifier = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "classifier.fit(x, y)\n",
        "\n",
        "plt.figure(figsize=(12,6))\n",
        "plot_tree(classifier, filled=True, feature_names=iris.feature_names, rounded=True, class_names=iris.target_names);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How does this work?\n",
        "\n",
        "The CART algorithm, used to train Decision Trees, works by splitting the training set into two subset using a single feature $d$ and a threshold $t$, which are chosen so that the split produces the **purest** subsets (weighted by size). Purity is to be intended in terms of the `criterion` selected.\n",
        "\n",
        "$$R_{left}(d,t) = R_{parent} \\cap \\{x : x_d \\leq t\\}$$\n",
        "$$R_{right}(d,t) = R_{parent} \\cap \\{x : x_d > t\\}$$\n",
        "\n",
        "It does this for every node recursively until impurity cannot be reduced anymore or the `max_depth` of the tree is reached. The classification task is then carried out with majority voting (or some other decision rule) at the level of leaves.\n",
        "\n",
        "**Note:** This is a greedy algorithm, meaning that the tree which is found is not necessarily optimal overall.\n",
        "\n",
        "There are several hyperparameters that control additional stopping conditions while learning the tree, essentially regularizing it: check in the [sklearn documentation](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). Let's also go back and check what happens if we don't regularize well our model."
      ],
      "metadata": {
        "id": "0d0dah_OzTsQ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8oD-KbOnpGj"
      },
      "source": [
        "Decision Trees are **easy to interpret** and that's why they are popular for several problems that requires explainability."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Instabilities\n",
        "\n",
        "One final (bad) note on decision trees. As you've seen Decision Trees operate by decision boundaries which are orthogonal to the axis. However this implies that some simple transformations to the representation of a problem (e.g. a rotation) can lead to difficulties in fitting a tree and instabilities. As an example:"
      ],
      "metadata": {
        "id": "plJbDiVa2Gc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(6)\n",
        "X_square = np.random.rand(100, 2) - 0.5\n",
        "y_square = (X_square[:, 0] > 0).astype(np.int64)\n",
        "\n",
        "angle = np.pi / 6  # 45 degrees\n",
        "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],\n",
        "                            [np.sin(angle), np.cos(angle)]])\n",
        "X_rotated_square = X_square.dot(rotation_matrix)\n",
        "\n",
        "tree_clf_square = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
        "tree_clf_square.fit(X_square, y_square)\n",
        "tree_clf_rotated_square = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
        "tree_clf_rotated_square.fit(X_rotated_square, y_square)\n",
        "\n",
        "plot_decision_surface(tree_clf_square, X_square, y_square)\n",
        "plot_decision_surface(tree_clf_rotated_square, X_rotated_square, y_square)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wskPc1bv2e8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fj7Ee9MMegWI"
      },
      "source": [
        "# C) Random Forests\n",
        "\n",
        "We saw during classes that the expected test error can be written as\n",
        "\n",
        "$${error} = {bias}^2 + variance + noise$$\n",
        "\n",
        "In general we cannot reduce the noise, we can reduce the bias increasing model complexity, but this makes the variance increase too.\n",
        "\n",
        "Can we reduce the variance without increasing the bias and without getting more data? **Yes**\n",
        "\n",
        "\n",
        "\n",
        "The idea is to take the average prediction of $K$ models.\n",
        "\n",
        "<img src=\"https://miro.medium.com/v2/resize:fit:696/1*7AxWhp2UMm7smg9ZmXlRHg.png\" width=500>\n",
        "\n",
        "In fact, if we consider our model to be a random variable $X$, the variance of the mean is lower than the variance of the population, i.e., ${Var}(\\overline X_K) \\le Var(X)$ (using the CLT: ${Var}(\\overline X_K) \\approx \\frac{Var(X)}{K})$.\n",
        "\n",
        "\n",
        "The problem is that we have only a single training set: we can use **bootstrapping** (i.e., sampling with replacement) to learn the K models from K sets of bootstrapped samples (note that this can be done in parallel). The prediction is then made by aggregating the predictions of all predictors (e.g. the mode for classification).\n",
        "\n",
        "This technique is known as **bagging** and works particularly well with trees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKtdaJB5txTz"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "model = RandomForestClassifier(n_estimators=5,\n",
        "                               random_state=42)\n",
        "model.fit(x_prime, y)\n",
        "\n",
        "plot_decision_surface(model, x_prime, y)\n",
        "plt.xlabel(features[features_used[0]])\n",
        "plt.ylabel(features[features_used[1]])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hyperparameters of Random Forest ](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) include mostly those needed for Decision Trees AND those relative to the ensemble (e.g. `n_estimators`, `bootstrap`)."
      ],
      "metadata": {
        "id": "7wxGDVoCBLLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature importance\n",
        "\n",
        "We've seen that one of the main pros of trees is their interpretability. Did we lose that going toward a forest?\n",
        "Not completely:"
      ],
      "metadata": {
        "id": "SSD2FmU95VEq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RandomForestClassifier(n_estimators=5,\n",
        "                               random_state=42)\n",
        "model.fit(x, y)\n",
        "\n",
        "importances = model.feature_importances_\n",
        "std = np.std([tree.feature_importances_ for tree in model.estimators_], axis=0)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "forest_importances = pd.Series(importances, index=features)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6,4))\n",
        "forest_importances.plot.bar(yerr=std, ax=ax)\n",
        "ax.set_title(\"Feature importances\")\n",
        "ax.set_ylabel(\"Mean decrease in impurity\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(alpha=0.3)\n",
        "fig.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iaOFrchP_sVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Obvsiouly we could also look at all the single trees of the forest but when they're many, feature importance convey information in a more intuitive and faster way."
      ],
      "metadata": {
        "id": "tdChnU0OFI-i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Ensemble learning"
      ],
      "metadata": {
        "id": "QJXqjis45hx6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you aggregate the predictions of a group of predictors, you will often get better predictions than with the best individual predictor. Such group is called ensemble, and this technique is called, in general, ensemble learning.\n",
        "\n",
        "Note that the predictors don't necessarily have to be of the same class, and in general the ensemble works best when the predictors are independent from one another, in fact diversity, even between trees in random forest for example, is desirable (notice that trees are learned with subset of features in many cases).\n",
        "\n",
        "Surprisingly even if all the predictors are weak learners, the ensemble might perform quite well!"
      ],
      "metadata": {
        "id": "z95XetLv9mkz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qU7FVNyyuRoQ"
      },
      "source": [
        "#### Pointers to some popular (different) ensemble methods\n",
        "\n",
        "* `xgboost`: https://xgboost.readthedocs.io/en/latest/\n",
        "* `lightgbm`: https://lightgbm.readthedocs.io/en/latest/\n",
        "\n",
        "Note: these methods are based on **boosting**, an approach based on the idea of using a set of weak learners to iteratively reduce the error. Just to give an intuition:\n",
        "\n",
        "<img src=\"https://pantelis.github.io/cs634/docs/common/lectures/ensemble/boosting/images/gradient-boost-example.png#center\" height=600>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Final remark:** Some of the models that you've seen today are actually very powerful and in some contexts (low to medium regime data, tabular data) can outperform more complex (and fancy) models. As an example, take a look at [this article](https://www.sciencedirect.com/science/article/pii/S0169207021001679), summarizing the results of one of the most important forecasting Kaggle competitions of the last years."
      ],
      "metadata": {
        "id": "VJyLsvp15syQ"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
