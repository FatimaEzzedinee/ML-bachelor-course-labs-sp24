{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/FatimaEzzedinee/ML-bachelor-course-labs-sp24/blob/main/02_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nu4HJeN686y2"
      },
      "source": [
        "# Machine Learning SP 2023/2024\n",
        "\n",
        "- Prof. Cesare Alippi\n",
        "- Alvise Dei Rossi ([`alvise.dei.rossi@usi.ch`](mailto:alvise.dei.rossi@usi.ch))<br>\n",
        "- Fatima Ezzeddine ([`fatima.ezzeddine@usi.ch`](mailto:fatima.ezzeddine@usi.ch))<br>\n",
        "- Alessandro Manenti ([`alessandro.manenti@usi.ch`](mailto:alessandro.manenti@usi.ch))\n",
        "\n",
        "---\n",
        "\n",
        "# Lab 02: Classification\n",
        "\n",
        "---\n",
        "\n",
        "# A) **Goal**: classify linearly separable synthetic data\n",
        "\n",
        "We have a $d$-dimensional input vector $X \\in \\mathbb{R}^d$ and a set of $k$ possible classes, $C_1, \\dots, C_k$.\n",
        "Our goal in classification is to assign $X$ to the correct class.\n",
        "\n",
        "In particular, our goal is to determine a __discriminant__ function to partition the input space. In this session we will focus on binary classification.\n",
        "\n",
        "![alt text](https://jakelearnsdatascience.files.wordpress.com/2017/02/lda_binary.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-r1hVAU5PFxC"
      },
      "source": [
        "# A.1) Collect and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IgAvdbC1482"
      },
      "outputs": [],
      "source": [
        "# first we define some helper functions to generate data and plot results\n",
        "import numpy as np\n",
        "from sklearn.datasets import make_classification, make_circles, make_moons\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"image.cmap\"] = \"bwr\" # Define the colors to use in the plot\n",
        "plt.rcParams[\"scatter.edgecolors\"] = 'k'\n",
        "\n",
        "\n",
        "# function to generate classification problems\n",
        "def get_data(n, ctype, noise_scale=1):\n",
        "  if ctype == 'simple':\n",
        "    x, y = make_classification(n_features=2, # we consider a simple problem with only 2 features (we can visualize them!)\n",
        "                               n_redundant=0, # there's no feature which is a linear combination of the informative features\n",
        "                               n_informative=2, # all features are informative\n",
        "                               n_clusters_per_class=1) # the same class is not divided in several clusters\n",
        "    x += np.random.uniform(size=x.shape)*noise_scale # adds some noise\n",
        "  elif ctype == 'circles':\n",
        "    x, y = make_circles(n, noise=0.1, factor=0.5) # the radius of the inner circle is half of the outer's\n",
        "  else:\n",
        "    raise ValueError\n",
        "  return x, y\n",
        "\n",
        "# function to plot decision boundaries\n",
        "def plot_decision_surface(model, x, y, transform=lambda x:x, with_probability=False):\n",
        "  #init figure\n",
        "  fig = plt.figure()\n",
        "\n",
        "  # Create mesh\n",
        "  h = .01  # step size in the mesh\n",
        "  # consider plotting bounds by looking at minimum and maximum for each feature\n",
        "  x1_min, x1_max = x[:, 0].min() - .5, x[:, 0].max() + .5\n",
        "  x2_min, x2_max = x[:, 1].min() - .5, x[:, 1].max() + .5\n",
        "  # we consider\n",
        "  xx, yy = np.meshgrid(np.arange(x1_min, x1_max, h),\n",
        "                        np.arange(x2_min, x2_max, h))\n",
        "\n",
        "  if with_probability == False:\n",
        "    y_pred = model.predict(transform(np.c_[xx.ravel(), yy.ravel()])) # we get predictions for all points in the grid\n",
        "\n",
        "    y_pred = y_pred.reshape(xx.shape)\n",
        "\n",
        "    # plot background color\n",
        "    plt.contourf(xx, yy, y_pred > 0.5, alpha=.5)\n",
        "  else:\n",
        "    y_proba = model.predict_proba(transform(np.c_[xx.ravel(), yy.ravel()]))\n",
        "\n",
        "    y_proba = 1- y_proba[:,0].reshape(xx.shape[0], xx.shape[1])\n",
        "\n",
        "    # plot background color\n",
        "    plt.imshow(y_proba, alpha=.5, extent=[x1_min, x1_max, x2_min, x2_max], origin='lower')\n",
        "    plt.colorbar(label='Probability of being RED')\n",
        "\n",
        "\n",
        "  # plot train data\n",
        "  plt.scatter(x[:, 0], x[:, 1], c=y) # color the points based on the labels\n",
        "\n",
        "  plt.xlim(xx.min(), xx.max())\n",
        "  plt.ylim(yy.min(), yy.max())\n",
        "\n",
        "  plt.xlabel(r'$x_1$')\n",
        "  plt.ylabel(r'$x_2$')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SjvNW28BxGR"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Create a classification problem\n",
        "x, y = get_data(100, ctype='simple')\n",
        "\n",
        "# Let's look at the data\n",
        "plt.scatter(x[:, 0], x[:, 1], c=y)\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfQqt52oPFxD"
      },
      "source": [
        "# A.2) Linear Discriminant Analysis\n",
        "\n",
        "Bayes' theorem states:\n",
        "$$\n",
        "P(A | B) = \\frac{P(B|A) P(A)}{P(B)}\n",
        "$$\n",
        "\n",
        "If A is the event \"$Y_i$ = k\"\n",
        "\n",
        "and B is the event \"$X_i$ = x\"\n",
        "\n",
        "we have seen in class that a **Bayes classifier** can be built:\n",
        "$$\n",
        "P(Y_i = k | X_i = x) = \\frac{P(X_i = x|Y_i = k) P(Y_i = k)}{P(X_i = x)}\n",
        "$$\n",
        "\n",
        "<br/><br/><br/><br/><br/>\n",
        "**Linear Discriminant Analysis (LDA)** makes 2 main assumptions on the data generating process:\n",
        "\n",
        "1) The likelihoods P($X_i$ = x|$Y_i$ = k) are Gaussian probability density functions\n",
        "\n",
        "2) All the Gaussians have the same covariance matrix $\\Sigma$\n",
        "\n",
        "This way Bayes' theorem can be written as:\n",
        "$$\n",
        "P(Y_i = k | X_i = x) = \\frac{\\pi_k f_k(x)}{\\Sigma_{l=1}^K \\pi_l f_l(x)}\n",
        "$$\n",
        "\n",
        "with\n",
        "$$\n",
        "f_k(x) = \\frac{1}{(2\\pi)^{p/2} |\\Sigma|^{1/2}} e^{-\\frac{1}{2}(x-\\mu)^T \\Sigma^{-1} (x-\\mu)}\n",
        "$$\n",
        "and\n",
        "$$\n",
        "\\pi_k = P(Y_i = k)\n",
        "$$\n",
        "\n",
        "The predicted class for $Y_i$ will be\n",
        "$$\n",
        "argmax_k(P(Y_i = k | X_i = x)) = argmax_k(log(P(Y_i = k | X_i = x)))\n",
        "$$\n",
        "\n",
        "If we compute $log(P(Y_i = k | X_i = x))$ (removing k-independent terms) we get:\n",
        "\n",
        "$$\n",
        "log(P(Y_i = k | X_i = x)) \\equiv \\boxed{\\delta_k(x)} = \\boxed{x^T \\Sigma^{-1}\\mu_k - \\frac{1}{2}\\mu_k^T\\Sigma^{-1}\\mu_k + log(\\pi_k)}\n",
        "$$\n",
        "\n",
        "**That is LINEAR** in x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMfWDH6UPFxE"
      },
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9MSATaDPFxE"
      },
      "outputs": [],
      "source": [
        "classifier = LinearDiscriminantAnalysis()\n",
        "classifier.fit(x, y)\n",
        "\n",
        "plot_decision_surface(classifier, x, y, with_probability=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The decision boundary of LDA is linear. In the example shown above LDA is able to descriminate perfectly between the two classes. What happens if you increase the noise in the data generating process? Try, and plot the decision boundary."
      ],
      "metadata": {
        "id": "orWNbZ_9CSu4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# Create a classification problem\n",
        "x_noisy, y_noisy = get_data(100, ctype='simple', ...)\n",
        "\n",
        "classifier = ...\n",
        "...\n",
        "\n",
        "plot_decision_surface(...)"
      ],
      "metadata": {
        "id": "IK-lzaJLCJPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lb6gDUxBPFxE"
      },
      "source": [
        "# A.3) Logistic Regression\n",
        "The Logistic Regression classifier makes an assumption on the posterior:\n",
        "\n",
        "$$\\boxed{Pr(y_i=1\\vert x_i, \\boldsymbol \\theta)} = \\sigma(x_i^\\top\\boldsymbol \\theta) = \\boxed{\\frac{1}{1+e^{-x_i^\\top\\boldsymbol \\theta}}}\n",
        "$$\n",
        "\n",
        "Where $\\sigma({}\\cdot{})$ is the _sigmoid_ function:\n",
        "\n",
        "<img style=\"text-align: center\" src=\"https://miro.medium.com/v2/resize:fit:970/1*Xu7B5y9gp0iL5ooBj7LtWw.png\" width=\"500\">\n",
        "\n",
        "For 1 sample the probability can be written as:\n",
        "$$\n",
        "Pr(y_i\\vert x_i, \\boldsymbol \\theta) = \\sigma_i^{y_i}(1 - \\sigma_i)^{1-y_i}\n",
        "$$\n",
        "\n",
        "For n samples the joint probability is:\n",
        "$$\n",
        "Pr(y_1, y_2, ..., y_n\\vert x_1, x_2, ..., x_n,  \\boldsymbol \\theta) = \\Pi_i^n\\sigma_i^{y_i}(1 - \\sigma_i)^{1-y_i}\n",
        "$$\n",
        "\n",
        "Maximizing the joint probability is equivalent to maximizing its logarithm:\n",
        "$$\n",
        "l(\\boldsymbol \\theta) = \\Sigma_i(y_ilog(\\sigma_i) + (1 - y_i)log(1 - \\sigma_i))\n",
        "$$\n",
        "\n",
        "and\n",
        "$$\n",
        "\\hat{\\theta} = argmax_{\\theta}(l(\\theta))\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayq8nv8xojjG"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "np.random.seed(42)\n",
        "x, y = get_data(100, ctype='simple')\n",
        "\n",
        "classifier = LogisticRegression(penalty=None) # create an instance of the model\n",
        "classifier.fit(x, y)              # fit the data\n",
        "\n",
        "plot_decision_surface(classifier, x, y, with_probability=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_4giXAY4TPV"
      },
      "source": [
        "Logistic Regression is often referred to as a **generalized linear method**. Indeed, even if the model is nonlinear, the predicted class depends only on the linear combination $x_i^\\top \\boldsymbol \\theta$ of the input variables. In other words, the decision surfaces are **linear**.\n",
        "\n",
        "Indeed, the decision boundary of the binary logistic regression model that we built can be written as:\n",
        "\n",
        "$$x_2 = -\\frac{\\theta_0}{\\theta_2} - \\frac{\\theta_1}{\\theta_2} x_1$$\n",
        "\n",
        "_Exercise: show why this is true (reminder: the decision boundary is the line where the probability of both classes is 0.5)._"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solution:"
      ],
      "metadata": {
        "id": "ZgTMg8r2IYK3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wi0MniB74Q7a"
      },
      "outputs": [],
      "source": [
        "print(f'Classifier Intercept: {classifier.intercept_}')\n",
        "print(f'Classifer Angular coeffs.: {classifier.coef_}')\n",
        "\n",
        "theta = [classifier.intercept_, classifier.coef_[0,0], classifier.coef_[0,1]]\n",
        "\n",
        "b = -theta[0]/theta[2]\n",
        "m = -theta[1]/theta[2]\n",
        "\n",
        "x1 = np.array([x[:,0].min(), x[:,0].max()])\n",
        "x2 = b + m * x1\n",
        "\n",
        "plot_decision_surface(classifier, x, y, with_probability=True)\n",
        "plt.plot(x1, x2, c='black')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVwVoxFoEi1F"
      },
      "source": [
        "---\n",
        "\n",
        "# B) **Goal**: classify non-linearly separable synthetic data\n",
        "\n",
        "# B.1) Prepare data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2rlSbxu8_jM5"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "x,y = get_data(120, 'circles')\n",
        "\n",
        "classifier = LogisticRegression(penalty=None)\n",
        "classifier.fit(x, y)\n",
        "\n",
        "plot_decision_surface(classifier, x, y, with_probability=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Task: How would you solve this problem?\n",
        "\n",
        "Can you think of another coordinate system that improves the representation for a model which can only linearly separate the data?"
      ],
      "metadata": {
        "id": "TM2Wg032JUdG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_fn():\n",
        "  ...\n",
        "\n",
        "\n",
        "# test if your data is now linearly separable\n",
        "x_tranformed = transform_fn(x)\n",
        "plt.scatter(x_transformed[:,0], x_transformed[:,1], c=y)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dgCtXKsBKwa6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cO6nKbEKIyRn"
      },
      "source": [
        "# B.2) Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-A5AKM1GAMah"
      },
      "outputs": [],
      "source": [
        "# Let's fit a model using the transformed features\n",
        "\n",
        "classifier = LogisticRegression(penalty=None)\n",
        "classifier.fit(x_transformed, y)\n",
        "\n",
        "plot_decision_surface(classifier, x, y, transform=transform_fn, with_probability=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IzlJtIhDJ4cO"
      },
      "source": [
        "It is not always easy to find suitable features/projections by hand.\n",
        "\n",
        "In a future Lab session we will see how to use neural networks to solve classification problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gttJ8kwLVdh"
      },
      "source": [
        "## K-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3FU_m-RhPFxF"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "np.random.seed(42)\n",
        "\n",
        "x_train, y_train = get_data(80, 'circles')\n",
        "x_validation, y_validation = get_data(50, 'circles')\n",
        "x_test, y_test = get_data(50, 'circles')\n",
        "\n",
        "# Let's look at the data\n",
        "plt.scatter(x_train[:, 0], x_train[:, 1], marker='o', c=y_train)\n",
        "plt.scatter(x_test[:, 0], x_test[:, 1], marker=\"v\", c=y_test)\n",
        "plt.scatter(x_validation[:, 0], x_validation[:, 1], marker=\"+\", c=y_validation)\n",
        "\n",
        "# Phantom plots for legend\n",
        "plt.scatter([],[],c=\"k\",marker=\"+\",label=\"Validation\")\n",
        "plt.scatter([],[],c=\"k\",marker=\"v\",label=\"Test\")\n",
        "plt.scatter([],[],c=\"k\",marker=\"o\",label=\"Train\")\n",
        "\n",
        "plt.xlabel(r'$x_1$')\n",
        "plt.ylabel(r'$x_2$')\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_78gGKI-PFxF"
      },
      "source": [
        "### Goal: find the optimal number $\\hat{k}$ of k-nn to use.\n",
        "\n",
        "Hint 1: look at the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
        "\n",
        "Hint 2: Remember what train, validation and test sets are used for\n",
        "\n",
        "\n",
        "\n",
        "| Set        | Usage                                                                                       |\n",
        "| ---------- | ------------------------------------------------------------------------------------------- |\n",
        "| Train      | used to fit your model                                                                      |\n",
        "| Validation | used to choose the best hyperparameters (those parameters you have to set at the beginning) |\n",
        "| Test       | used only once to have an estimate of your model's performance                              |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "igyFScSyLehY"
      },
      "outputs": [],
      "source": [
        "# try it on your own!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}